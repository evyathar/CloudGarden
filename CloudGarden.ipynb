{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YairZen/CloudProject_Unicorn/blob/lior/HW3_Unicorn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxU3y_ahSBCr"
      },
      "source": [
        "‚úÖ PIP INSTALL (GLOBAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD6QB2FYWlkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d477d7e0-c450-49a3-b4e1-c8a04c67d6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade gradio pandas matplotlib python-docx\n",
        "!pip install -q --upgrade firebase-admin plotly gdown\n",
        "!pip -q install cerebras-cloud-sdk\n",
        "!pip -q install -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXiHxUFpa556"
      },
      "source": [
        "‚úÖ IMPORTS (GLOBAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkZpGdnxM6az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7f2651-441e-4a40-fb22-174511d263f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#######\n",
        "# Report Generator with DOCX Export\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "from zoneinfo import ZoneInfo\n",
        "from datetime import timezone\n",
        "from docx import Document\n",
        "import tempfile\n",
        "import os\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.shared import Inches, Pt, RGBColor\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict\n",
        "\n",
        "#Gamification imports\n",
        "import random\n",
        "\n",
        "# Firebase imports\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "\n",
        "# Additional imports for IoT Dashboard\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from datetime import timedelta\n",
        "import gdown\n",
        "import json\n",
        "\n",
        "# Plotly imports\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#--------RAG & Index-------------#\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from urllib.parse import quote\n",
        "from typing import Tuple, Any, Optional\n",
        "# --- Web fetch + HTML parsing ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- NLP (stemming / stopwords) ---\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "# --- Chat conversation ---#\n",
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSPgJzDMOn_M"
      },
      "source": [
        "Firebase & API Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGLzSlUNOmJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c690ecc-1311-49c7-92d3-1bbbf564f79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Downloading Firebase credentials...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ESnh8BIbGKrVEijA9nKNgNJNdD5kAaYC\n",
            "To: /content/firebase_key.json\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.37k/2.37k [00:00<00:00, 7.87MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Project: cloud-81451\n",
            " Firebase initialized\n",
            " Firebase configured\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#CEREBRAS cardetntial\n",
        "os.environ[\"CEREBRAS_API_KEY\"] =\"csk-r8npfcy9jckcxcd98t4422mw99wx3ew89k4h3rrhdvy5ekde\"\n",
        "client = Cerebras(api_key=os.environ[\"CEREBRAS_API_KEY\"])\n",
        "REPORT_MODEL_NAME = \"llama3.1-8b\"\n",
        "\n",
        "# Firebase credentials\n",
        "FIREBASE_KEY_ID = '1ESnh8BIbGKrVEijA9nKNgNJNdD5kAaYC'\n",
        "firebase_key_file = 'firebase_key.json'\n",
        "#add for the RAG & INDEX CODE#\n",
        "FIREBASE_URL = \"https://cloud-81451-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
        "#-----#\n",
        "if os.path.exists(firebase_key_file):\n",
        "    os.remove(firebase_key_file)\n",
        "\n",
        "print(' Downloading Firebase credentials...')\n",
        "try:\n",
        "    url = f'https://drive.google.com/uc?id={FIREBASE_KEY_ID}'\n",
        "    gdown.download(url, firebase_key_file, quiet=False, fuzzy=True)\n",
        "    with open(firebase_key_file, 'r') as f:\n",
        "        creds = json.load(f)\n",
        "    print(f'‚úì Project: {creds.get(\"project_id\")}')\n",
        "except Exception as e:\n",
        "    print(f'‚ö†Ô∏è Error: {e}')\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        os.rename(list(uploaded.keys())[0], firebase_key_file)\n",
        "\n",
        "# Initialize Firebase\n",
        "if not firebase_admin._apps:\n",
        "    firebase_admin.initialize_app(\n",
        "        credentials.Certificate(firebase_key_file),\n",
        "        {'databaseURL': 'https://cloud-81451-default-rtdb.europe-west1.firebasedatabase.app/'}\n",
        "    )\n",
        "    print(' Firebase initialized')\n",
        "\n",
        "# Server Configuration (already exists in Cell 6, but adding here for completeness)\n",
        "BATCH_LIMIT = 200\n",
        "\n",
        "print(' Firebase configured')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3VvQJgmOu6H"
      },
      "source": [
        "## üîÑ Firebase Sync Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXeOpqeCOwvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f301403f-5af7-46ea-a537-bbae1c3c9e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sync functions loaded\n"
          ]
        }
      ],
      "source": [
        "# Sync Functions\n",
        "\n",
        "def get_latest_timestamp_from_firebase():\n",
        "    try:\n",
        "        latest = db.reference('/sensor_data').order_by_child('created_at').limit_to_last(1).get()\n",
        "        return list(latest.values())[0]['created_at'] if latest else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def fetch_batch_from_server(before_timestamp=None):\n",
        "    params = {\"feed\": FEED, \"limit\": BATCH_LIMIT}\n",
        "    if before_timestamp:\n",
        "        params[\"before_created_at\"] = before_timestamp\n",
        "    try:\n",
        "        return requests.get(f\"{BASE_URL}/history\", params=params, timeout=180).json()\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "def save_sensor_data_to_firebase(data_list):\n",
        "    if not data_list:\n",
        "        return 0\n",
        "\n",
        "    ref = db.reference('/sensor_data')\n",
        "    saved = 0\n",
        "\n",
        "    for sample in data_list:\n",
        "        try:\n",
        "            vals = json.loads(sample['value'])\n",
        "            temperature = max(-50, min(100, float(vals['temperature'])))\n",
        "            humidity = max(0, min(100, float(vals['humidity'])))\n",
        "            soil = max(0, min(100, float(vals['soil'])))\n",
        "            timestamp_key = sample['created_at'].replace(':', '-').replace('.', '-')\n",
        "\n",
        "            ref.child(timestamp_key).set({\n",
        "                'created_at': sample['created_at'],\n",
        "                'temperature': temperature,\n",
        "                'humidity': humidity,\n",
        "                'soil': soil\n",
        "            })\n",
        "\n",
        "            saved += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return saved\n",
        "\n",
        "def sync_new_data_from_server():\n",
        "    msgs = [\"Starting sync...\"]\n",
        "    latest = get_latest_timestamp_from_firebase()\n",
        "    msgs.append(f\"Latest: {latest}\" if latest else \"No existing data\")\n",
        "    resp = fetch_batch_from_server()\n",
        "\n",
        "    if \"data\" not in resp:\n",
        "        return \"\\n\".join(msgs + [\"Error fetching data\"]), 0\n",
        "\n",
        "    new = [s for s in resp[\"data\"] if not latest or s[\"created_at\"] > latest]\n",
        "\n",
        "    if new:\n",
        "        saved = save_sensor_data_to_firebase(new)\n",
        "        return \"\\n\".join(msgs + [f\"Found {len(new)} new\", f\"Saved {saved}!\"]), saved\n",
        "\n",
        "    return \"\\n\".join(msgs + [\"No new data\"]), 0\n",
        "\n",
        "print('Sync functions loaded')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Vu19SqO2Ic"
      },
      "source": [
        "## üìä Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdmkuJAZO3iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bede8a35-e503-4594-caee-27630009669e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data loading ready\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Data Loading (YOUR ORIGINAL CODE)\n",
        "\n",
        "def load_data_from_firebase():\n",
        "    data = db.reference('/sensor_data').get()\n",
        "    if not data:\n",
        "        return pd.DataFrame()\n",
        "    df = pd.DataFrame([{\n",
        "        'timestamp': pd.to_datetime(v['created_at']),\n",
        "        'temperature': float(v['temperature']),\n",
        "        'humidity': float(v['humidity']),\n",
        "        'soil': float(v['soil'])\n",
        "    } for v in data.values()])\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['humidity'] = df['humidity'].clip(0, 100)\n",
        "    df['soil'] = df['soil'].clip(0, 100)\n",
        "    df['temperature'] = df['temperature'].clip(-50, 100)\n",
        "    return df\n",
        "\n",
        "print('‚úÖ Data loading ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJqc9RlyNbcN"
      },
      "source": [
        "‚úÖ GLOBAL CONFIG / THEME / CSS (OPTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz7Tm5DJM7hn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "726ab947ca294c58ab93eaee9826f017",
            "84be023da3aa417ca98ef6f34f13adec",
            "064c7e935ce944eebd45e60b15ab489d",
            "758386d0c2e64d46bec8f132ddbcf2a6",
            "274ff0c1d2aa464c8d82300fae161199",
            "1b9b0572b39647efbbd1df07695a48ba",
            "68e6d5f37abe47648648788e069e0ae8",
            "3a7c978b23d04313a85a1d2e6c10b1eb",
            "c209eb7772c449a9b19c4b92b09e9f54",
            "042dcef774e74347967b23293328a131",
            "e03c6f4feadb43388d529cbf3e6f34f2",
            "3fda48b9e2974569a0923c88e90b70b1",
            "443965819c2d4c169e4f669ded6fa253",
            "b1ce56b70ab5482f82c61c840c97a99a",
            "31e1ea2f904142908e3a3f4baa64d442",
            "749925a9650f443fa460ec5086d2d0c8",
            "4198ddd5cddb46cb86e2e66233f9314e",
            "42193c69ee4146eca44720b135a888e1",
            "223d2ec616274bdd955d5ef4fcdedfd7",
            "90d8fbceefc34347b1275add4f79511d",
            "1eea0809ed6440098d172b15de410a3d",
            "6d282e036f1041dd902818fc101e21d6",
            "6e7bd1e1b65440db94631964cc87dac8",
            "2c594239619b410e8d29ecbb72e3690e",
            "ad0d7ffae00c4e9dafb9dfd9ebb458f4",
            "421024c3b59c4721b266d8951e2124d2",
            "3ceca0ed139644f5931a71718ffe2c8a",
            "bf7202ec49b04ae79b23283298590b33",
            "5b46a4e5d81d4d3a9cf0f3be5afab6c0",
            "c0a083473fc648a4a86b0e6e3466fb70",
            "1f862738dbc04427abbbb222b8eb34b0",
            "5c58ebb4a1474e9b85ec8a0e35cf31f9",
            "f1cd76e2ea85440fa806169eb9371b2b"
          ]
        },
        "outputId": "8043b9f9-1302-4d0d-b76e-0c4af899f773"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726ab947ca294c58ab93eaee9826f017"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/9.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fda48b9e2974569a0923c88e90b70b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/408 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e7bd1e1b65440db94631964cc87dac8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded\n",
            "üì° Server: https://server-cloud-v645.onrender.com/\n",
            "üìª Feed: json\n",
            "üì¶ Batch limit: 200\n"
          ]
        }
      ],
      "source": [
        "# Global Configuration\n",
        "\n",
        "# ============================================================================\n",
        "# SERVER & FEED CONFIGURATION (YOUR ORIGINAL SETTINGS)\n",
        "# ============================================================================\n",
        "FEED = \"json\"  # Your feed name - CHANGE THIS if different\n",
        "BASE_URL = \"https://server-cloud-v645.onrender.com/\"\n",
        "BATCH_LIMIT = 200\n",
        "\n",
        "# ============================================================================\n",
        "# APP CONFIGURATION\n",
        "# ============================================================================\n",
        "APP_TITLE = \"üå± CloudGarden\"\n",
        "APP_SUBTITLE = \"Smart Plant Disease Detection System\"\n",
        "\n",
        "# --- Colors for Friend's Realtime Dashboard ---\n",
        "COLOR_TEMP = \"#1f77b4\"   # blue\n",
        "COLOR_HUM  = \"#ff7f0e\"   # orange\n",
        "COLOR_SOIL = \"#2ca02c\"   # green\n",
        "\n",
        "STATUS_OK_COLOR = \"#2ca02c\"      # green\n",
        "STATUS_WARN_COLOR = \"#ffbf00\"    # yellow\n",
        "STATUS_BAD_COLOR = \"#d62728\"     # red\n",
        "\n",
        "# --- ML Model Configuration ---\n",
        "MODEL_NAME = \"linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification\"\n",
        "clf = pipeline(\"image-classification\", model=MODEL_NAME)\n",
        "\n",
        "# ============================================================================\n",
        "# CSS STYLING FOR IOT DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "COLORS = {\n",
        "    'temperature': {'color': '#ef4444'},\n",
        "    'humidity': {'color': '#3b82f6'},\n",
        "    'soil': {'color': '#8b5cf6'}\n",
        "}\n",
        "\n",
        "CUSTOM_CSS = \"\"\"\n",
        "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');\n",
        "* { font-family: 'Inter', sans-serif; }\n",
        "\n",
        ".kpi-card {\n",
        "    background: white;\n",
        "    padding: 24px;\n",
        "    border-radius: 12px;\n",
        "    box-shadow: 0 1px 3px rgba(0,0,0,0.12);\n",
        "    text-align: center;\n",
        "    border-left: 4px solid;\n",
        "}\n",
        ".kpi-label { color: #6b7280; font-size: 14px; font-weight: 600; }\n",
        ".kpi-value { font-size: 48px; font-weight: 700; color: #1f2937; }\n",
        ".trend-up { color: #10b981; }\n",
        ".trend-down { color: #ef4444; }\n",
        "\n",
        "/* Tooltip styles */\n",
        ".info-icon {\n",
        "    position: relative;\n",
        "    display: inline-flex;\n",
        "    cursor: help;\n",
        "}\n",
        "\n",
        ".info-icon .tooltip-text {\n",
        "    visibility: hidden;\n",
        "    width: 200px;\n",
        "    background-color: #1f2937;\n",
        "    color: white;\n",
        "    text-align: center;\n",
        "    border-radius: 6px;\n",
        "    padding: 8px;\n",
        "    position: absolute;\n",
        "    z-index: 1000;\n",
        "    bottom: 125%;\n",
        "    left: 50%;\n",
        "    margin-left: -100px;\n",
        "    opacity: 0;\n",
        "    transition: opacity 0.3s;\n",
        "    font-size: 11px;\n",
        "    line-height: 1.4;\n",
        "    box-shadow: 0 2px 8px rgba(0,0,0,0.2);\n",
        "}\n",
        "\n",
        ".info-icon .tooltip-text::after {\n",
        "    content: \"\";\n",
        "    position: absolute;\n",
        "    top: 100%;\n",
        "    left: 50%;\n",
        "    margin-left: -5px;\n",
        "    border-width: 5px;\n",
        "    border-style: solid;\n",
        "    border-color: #1f2937 transparent transparent transparent;\n",
        "}\n",
        "\n",
        ".info-icon:hover .tooltip-text {\n",
        "    visibility: visible;\n",
        "    opacity: 1;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print('‚úÖ Configuration loaded')\n",
        "print(f'üì° Server: {BASE_URL}')\n",
        "print(f'üìª Feed: {FEED}')\n",
        "print(f'üì¶ Batch limit: {BATCH_LIMIT}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISU3vcbDNf1p"
      },
      "source": [
        "‚úÖ TAB REGISTRY (MODULAR)\n",
        "\n",
        " ◊û◊ï◊°◊ô◊§◊ô◊ù ◊ó◊ú◊ï◊†◊ô◊™ ◊ó◊ì◊©◊î ◊®◊ß ◊¢\"◊ô:\n",
        " 1) ◊î◊ï◊°◊§◊™ TAB1 Logic - ◊õ◊ú ◊î◊§◊ï◊†◊ß◊¶◊ô◊ï◊™ ◊¢◊ñ◊® ◊ú◊û◊ô◊†◊î◊ù.\n",
        " 2) ◊î◊ï◊°◊§◊™ Tab1 GUI - ◊õ◊ú GRADIO\n",
        " * ◊ô◊© ◊ú◊î◊ï◊®◊ô◊ì ◊ê◊™ \"with gr.Blocks() as demo:\" ◊ï \"demo.launch()\"\n",
        "\n",
        " 3) ◊ú◊î◊ï◊°◊ô◊£ ◊ê◊™ ◊©◊ù ◊î◊§◊ï◊†◊ß◊¶◊ô◊ô◊™ GUI ◊ú◊®◊©◊ô◊û◊™ ◊îTAB ◊ú◊û◊ò◊î."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNQNqiEsWqfZ"
      },
      "source": [
        "‚úÖ TAB 1 Logic -  üå± Realtime Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNh-iF2WWqLB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------- Core Data Fetch ----------\n",
        "def load_iot_data(feed: str, limit: int) -> pd.DataFrame | None:\n",
        "    resp = requests.get(\n",
        "        f\"{BASE_URL}/history\",\n",
        "        params={\"feed\": feed, \"limit\": limit},\n",
        "        timeout=30\n",
        "    )\n",
        "    data = resp.json()\n",
        "    if \"data\" not in data or not data[\"data\"]:\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(data[\"data\"])\n",
        "    if \"created_at\" not in df.columns or \"value\" not in df.columns:\n",
        "        return None\n",
        "\n",
        "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n",
        "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"created_at\", \"value\"]).sort_values(\"created_at\")\n",
        "\n",
        "    return None if df.empty else df\n",
        "\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def normalize(series: pd.Series) -> pd.Series:\n",
        "    mn, mx = float(series.min()), float(series.max())\n",
        "    if mx - mn == 0:\n",
        "        return series * 0.0\n",
        "    return (series - mn) / (mx - mn)\n",
        "\n",
        "\n",
        "# ---------- Plant Status + Plots ----------\n",
        "def plant_dashboard(limit: int):\n",
        "    try:\n",
        "        dfs = {\n",
        "            \"temperature\": load_iot_data(\"temperature\", limit),\n",
        "            \"humidity\": load_iot_data(\"humidity\", limit),\n",
        "            \"soil\": load_iot_data(\"soil\", limit),\n",
        "        }\n",
        "\n",
        "        missing = [k for k, v in dfs.items() if v is None]\n",
        "        if missing:\n",
        "            return \"‚ö†Ô∏è Partial Data\", f\"Missing sensors or empty history: {', '.join(missing)}\", None, None, None, None\n",
        "\n",
        "        temp = float(dfs[\"temperature\"][\"value\"].iloc[-1])\n",
        "        hum = float(dfs[\"humidity\"][\"value\"].iloc[-1])\n",
        "        soil = float(dfs[\"soil\"][\"value\"].iloc[-1])\n",
        "\n",
        "        issues, warnings = [], []\n",
        "\n",
        "        checks = [\n",
        "            (\"Temperature\", temp, 18, 32, 1),\n",
        "            (\"Air humidity\", hum, 35, 75, 3),\n",
        "            (\"Soil moisture\", soil, 20, 60, 3),\n",
        "        ]\n",
        "\n",
        "        for name, value, low, high, margin in checks:\n",
        "            if not (low <= value <= high):\n",
        "                issues.append(f\"{name} out of range ({value:.1f})\")\n",
        "            elif value <= low + margin or value >= high - margin:\n",
        "                warnings.append(f\"{name} near limit ({value:.1f})\")\n",
        "\n",
        "        if issues:\n",
        "            status = \"üî¥ Plant Status: Not OK\"\n",
        "            details_main = \" ; \".join(issues)\n",
        "\n",
        "        elif warnings:\n",
        "            status = \"üü° Plant Status: Warning\"\n",
        "            details_main = \" ; \".join(warnings)\n",
        "\n",
        "        else:\n",
        "            status = \"üü¢ Plant Status: OK\"\n",
        "            details_main = \"All sensors are within valid ranges\"\n",
        "\n",
        "        details = (\n",
        "    f\"{details_main}\\n\"\n",
        "    f\"Latest values:\\n\"\n",
        "    f\"temp={temp:.1f}\\n\"\n",
        "    f\"humidity={hum:.1f}\\n\"\n",
        "    f\"soil={soil:.1f}\"\n",
        "\n",
        "        )\n",
        "\n",
        "        df_t, df_h, df_s = dfs[\"temperature\"], dfs[\"humidity\"], dfs[\"soil\"]\n",
        "\n",
        "        fig_t = plt.figure(figsize=(7, 3.2))\n",
        "        plt.plot(df_t[\"created_at\"], df_t[\"value\"], marker=\"o\", color=COLOR_TEMP)\n",
        "        plt.title(\"Temperature History\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"¬∞C\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        fig_h = plt.figure(figsize=(7, 3.2))\n",
        "        plt.plot(df_h[\"created_at\"], df_h[\"value\"], marker=\"o\", color=COLOR_HUM)\n",
        "        plt.title(\"Air Humidity History\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"%\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        fig_s = plt.figure(figsize=(7, 3.2))\n",
        "        plt.plot(df_s[\"created_at\"], df_s[\"value\"], marker=\"o\", color=COLOR_SOIL)\n",
        "        plt.title(\"Soil Moisture History\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"%\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        fig_c = plt.figure(figsize=(10, 3.4))\n",
        "        plt.plot(df_t[\"created_at\"], normalize(df_t[\"value\"]), marker=\"o\", label=\"Temperature (norm)\", color=COLOR_TEMP)\n",
        "        plt.plot(df_h[\"created_at\"], normalize(df_h[\"value\"]), marker=\"o\", label=\"Humidity (norm)\", color=COLOR_HUM)\n",
        "        plt.plot(df_s[\"created_at\"], normalize(df_s[\"value\"]), marker=\"o\", label=\"Soil (norm)\", color=COLOR_SOIL)\n",
        "        plt.title(\"Combined Trend (Normalized)\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Normalized Value (0‚Äì1)\")\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        return status, details, fig_t, fig_h, fig_s, fig_c\n",
        "\n",
        "    except Exception:\n",
        "        return \"‚ùå Error\", \"Failed to fetch data from server. Please try again.\", None, None, None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbBhAJeHXGSH"
      },
      "source": [
        "‚úÖ Tab 1 GUI - - üå± Realtime Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw1loVKKYxN0"
      },
      "outputs": [],
      "source": [
        "def build_realtime_dashboard_tab():\n",
        "    gr.Markdown(\n",
        "    \"<h3 style='margin:0; font-size:22px;'>üåø Overall Plant Status (Real-Time)</h3>\"\n",
        ")\n",
        "\n",
        "\n",
        "    samples = gr.Slider(1, 200, value=20, step=1, label=\"Number of Samples (used for all graphs)\")\n",
        "    overall_btn = gr.Button(\"Update Plant Dashboard\", variant=\"primary\")\n",
        "\n",
        "\n",
        "\n",
        "    overall_status = gr.Textbox(\n",
        "        label=\"Overall Status\",\n",
        "        lines=1,\n",
        "        placeholder=\"Click 'Update Plant Dashboard' to evaluate plant status\"\n",
        "    )\n",
        "    overall_info = gr.Textbox(\n",
        "        label=\"Status Details\",\n",
        "        lines=4,\n",
        "        placeholder=\"Detailed plant analysis will appear here\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(f\"\"\"\n",
        "<div class=\"legend-card\" style=\"margin-top:14px;padding:14px;border:1px solid var(--border-color-primary)\n",
        ";border-radius:10px;\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  <h4 style=\"margin-bottom:10px; font-size:20px; font-weight:600;\">\n",
        "üåø Plant Status\n",
        "</h4>\n",
        "\n",
        "\n",
        "  <span style=\"color:{STATUS_OK_COLOR};font-size:26px;\">‚óè</span>\n",
        "  <b>Healthy</b> ‚Äì All sensor values within normal ranges<br>\n",
        "\n",
        "  <span style=\"color:{STATUS_WARN_COLOR};font-size:26px;\">‚óè</span>\n",
        "  <b>Warning</b> ‚Äì At least one value near threshold<br>\n",
        "\n",
        "  <span style=\"color:{STATUS_BAD_COLOR};font-size:26px;\">‚óè</span>\n",
        "  <b>Not OK</b> ‚Äì One or more values out of range<br><br>\n",
        "\n",
        "  <span>Status is calculated automatically from sensor data</span>\n",
        "</div>\n",
        "        \"\"\")\n",
        "\n",
        "        gr.Markdown(f\"\"\"\n",
        "<div class=\"legend-card\" style=\"margin-top:14px;padding:14px;border:1px solid var(--border-color-primary)\n",
        ";border-radius:10px;\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  <h4 style=\"margin-bottom:10px; font-size:20px; font-weight:600;\">\n",
        "‚ÑπÔ∏è Valid Value Ranges\n",
        "</h4>\n",
        "\n",
        "\n",
        "  <span style=\"color:{COLOR_TEMP};font-size:26px;\">‚óè</span>\n",
        "  üå°Ô∏è <b>Temperature</b>: 18‚Äì32¬∞C<br>\n",
        "\n",
        "  <span style=\"color:{COLOR_HUM};font-size:26px;\">‚óè</span>\n",
        "  üíß <b>Air Humidity</b>: 35‚Äì75%<br>\n",
        "\n",
        "  <span style=\"color:{COLOR_SOIL};font-size:26px;\">‚óè</span>\n",
        "  üå± <b>Soil Moisture</b>: 20‚Äì60%<br><br>\n",
        "\n",
        "  <span>‚ö†Ô∏è Values outside these ranges are considered abnormal</span>\n",
        "</div>\n",
        "        \"\"\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "<h2 style=\"text-align:center; margin-top:22px; font-size:26px; font-weight:600;\">\n",
        "üìà Plant Sensor Graphs\n",
        "</h2>\n",
        "\"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        plot_temp = gr.Plot(label=\"Temperature\")\n",
        "        plot_hum = gr.Plot(label=\"Air Humidity\")\n",
        "\n",
        "    with gr.Row():\n",
        "        plot_soil = gr.Plot(label=\"Soil Moisture\")\n",
        "        plot_combined = gr.Plot(label=\"Combined (Normalized)\")\n",
        "\n",
        "    overall_btn.click(\n",
        "        fn=plant_dashboard,\n",
        "        inputs=[samples],\n",
        "        outputs=[overall_status, overall_info, plot_temp, plot_hum, plot_soil, plot_combined]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diLmmpb6NPNq"
      },
      "source": [
        "‚úÖ TAB 3 LOGIC - üìÑ Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4xGuzaSNQXh"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# TAB 1 ‚Äî Generate Report (LOGIC ONLY) | Cerebras LM | ENGLISH\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "def unify_sensor_dfs(dfs: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert temperature/humidity/soil dfs into one DataFrame:\n",
        "    timestamp | temperature | humidity | soil\n",
        "    \"\"\"\n",
        "    def prep(df, col):\n",
        "        if df is None or df.empty:\n",
        "            return pd.DataFrame(columns=[\"timestamp\", col])\n",
        "\n",
        "        out = df.copy()\n",
        "        if \"timestamp\" not in out.columns and \"created_at\" in out.columns:\n",
        "            out = out.rename(columns={\"created_at\": \"timestamp\"})\n",
        "\n",
        "        # timestamp may be an index\n",
        "        if \"timestamp\" not in out.columns:\n",
        "            if out.index.name is not None:\n",
        "                out = out.reset_index()\n",
        "            else:\n",
        "                out = out.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
        "\n",
        "        if \"timestamp\" not in out.columns or \"value\" not in out.columns:\n",
        "            return pd.DataFrame(columns=[\"timestamp\", col])\n",
        "\n",
        "        out = out[[\"timestamp\", \"value\"]]\n",
        "        ts = out[\"timestamp\"]\n",
        "\n",
        "        # numeric timestamp (s/ms) OR datetime string\n",
        "        if pd.api.types.is_numeric_dtype(ts) or ts.astype(str).str.fullmatch(r\"\\d+\").all():\n",
        "            ts_num = pd.to_numeric(ts, errors=\"coerce\")\n",
        "            unit = \"ms\" if ts_num.dropna().astype(int).astype(str).str.len().median() >= 13 else \"s\"\n",
        "            out[\"timestamp\"] = (\n",
        "                pd.to_datetime(ts_num, errors=\"coerce\", unit=unit, utc=True)\n",
        "                .dt.tz_convert(\"Asia/Jerusalem\")\n",
        "                .dt.tz_localize(None)\n",
        "            )\n",
        "        else:\n",
        "            out[\"timestamp\"] = (\n",
        "                pd.to_datetime(ts, errors=\"coerce\", utc=True)\n",
        "                .dt.tz_convert(\"Asia/Jerusalem\")\n",
        "                .dt.tz_localize(None)\n",
        "            )\n",
        "\n",
        "        out = out.dropna(subset=[\"timestamp\"])\n",
        "        out[\"value\"] = pd.to_numeric(out[\"value\"], errors=\"coerce\")\n",
        "        out = out.dropna(subset=[\"value\"])\n",
        "\n",
        "        out = out.rename(columns={\"value\": col})\n",
        "        return out\n",
        "\n",
        "    t = prep(dfs.get(\"temperature\"), \"temperature\")\n",
        "    h = prep(dfs.get(\"humidity\"), \"humidity\")\n",
        "    s = prep(dfs.get(\"soil\"), \"soil\")\n",
        "\n",
        "    df = t.merge(h, on=\"timestamp\", how=\"outer\").merge(s, on=\"timestamp\", how=\"outer\")\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "class AutomatedReportGenerator:\n",
        "    \"\"\"\n",
        "    Generate professional reports using AI (Cerebras LM).\n",
        "    Creates daily summaries and Word documents.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cerebras_client, model_name):\n",
        "        self.client = cerebras_client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate_daily_report(self, df: pd.DataFrame) -> str:\n",
        "        \"\"\"Generate AI-powered daily summary (ENGLISH).\"\"\"\n",
        "        if df.empty:\n",
        "            return \"No data available for report.\"\n",
        "\n",
        "        # Get last 24 hours or last 100 readings\n",
        "        try:\n",
        "            cutoff = df[\"timestamp\"].max() - timedelta(hours=24)\n",
        "            daily = df[df[\"timestamp\"] > cutoff]\n",
        "            if daily.empty:\n",
        "                daily = df.tail(100)\n",
        "        except:\n",
        "            daily = df.tail(100)\n",
        "\n",
        "        # Calculate statistics\n",
        "        stats = {\n",
        "            \"date\": daily[\"timestamp\"].max().strftime(\"%Y-%m-%d\"),\n",
        "            \"readings\": len(daily),\n",
        "            \"temp_avg\": daily[\"temperature\"].mean(),\n",
        "            \"temp_min\": daily[\"temperature\"].min(),\n",
        "            \"temp_max\": daily[\"temperature\"].max(),\n",
        "            \"humidity_avg\": daily[\"humidity\"].mean(),\n",
        "            \"humidity_min\": daily[\"humidity\"].min(),\n",
        "            \"humidity_max\": daily[\"humidity\"].max(),\n",
        "            \"soil_avg\": daily[\"soil\"].mean(),\n",
        "            \"soil_min\": daily[\"soil\"].min(),\n",
        "            \"soil_max\": daily[\"soil\"].max(),\n",
        "        }\n",
        "\n",
        "        # Build AI prompt (ENGLISH)\n",
        "        prompt = f\"\"\"Generate a professional daily plant health report based on this data:\n",
        "\n",
        "DATE: {stats['date']}\n",
        "READINGS: {stats['readings']} sensor measurements\n",
        "\n",
        "ENVIRONMENTAL CONDITIONS:\n",
        "- Temperature: {stats['temp_avg']:.1f}¬∞C (range: {stats['temp_min']:.1f}-{stats['temp_max']:.1f}¬∞C)\n",
        "- Humidity: {stats['humidity_avg']:.1f}% (range: {stats['humidity_min']:.1f}-{stats['humidity_max']:.1f}%)\n",
        "- Soil Moisture: {stats['soil_avg']:.1f}% (range: {stats['soil_min']:.1f}-{stats['soil_max']:.1f}%)\n",
        "\n",
        "Generate a concise daily summary (3-4 paragraphs in English) covering:\n",
        "1) Overall environmental conditions\n",
        "2) Risks and potential stress/disease\n",
        "3) Practical care recommendations\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an agricultural consultant generating plant health reports in English.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=800,\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"Error generating report: {str(e)}\"\n",
        "\n",
        "    def create_docx_report(self, df: pd.DataFrame, output_path: str) -> str:\n",
        "        \"\"\"Create formatted Word document report.\"\"\"\n",
        "        doc = Document()\n",
        "\n",
        "        # Title\n",
        "        title = doc.add_heading(\"üå± Daily Plant Health Report\", 0)\n",
        "        title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "        # Date\n",
        "        date_para = doc.add_paragraph()\n",
        "        date_run = date_para.add_run(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
        "        date_run.bold = True\n",
        "        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "        # Executive Summary (AI)\n",
        "        doc.add_heading(\"Executive Summary\", 1)\n",
        "        ai_summary = self.generate_daily_report(df)\n",
        "        doc.add_paragraph(ai_summary)\n",
        "\n",
        "        # Environmental Conditions Table\n",
        "        doc.add_heading(\"Environmental Conditions\", 1)\n",
        "\n",
        "        if not df.empty:\n",
        "            daily = df.tail(100)\n",
        "\n",
        "            table = doc.add_table(rows=4, cols=4)\n",
        "            table.style = \"Light Grid Accent 1\"\n",
        "\n",
        "            headers = table.rows[0].cells\n",
        "            headers[0].text = \"Parameter\"\n",
        "            headers[1].text = \"Current\"\n",
        "            headers[2].text = \"Average\"\n",
        "            headers[3].text = \"Range\"\n",
        "\n",
        "            # Temperature\n",
        "            row1 = table.rows[1].cells\n",
        "            row1[0].text = \"üå°Ô∏è Temperature\"\n",
        "            row1[1].text = f\"{daily['temperature'].iloc[-1]:.1f}¬∞C\"\n",
        "            row1[2].text = f\"{daily['temperature'].mean():.1f}¬∞C\"\n",
        "            row1[3].text = f\"{daily['temperature'].min():.1f}-{daily['temperature'].max():.1f}¬∞C\"\n",
        "\n",
        "            # Humidity\n",
        "            row2 = table.rows[2].cells\n",
        "            row2[0].text = \"üíß Humidity\"\n",
        "            row2[1].text = f\"{daily['humidity'].iloc[-1]:.1f}%\"\n",
        "            row2[2].text = f\"{daily['humidity'].mean():.1f}%\"\n",
        "            row2[3].text = f\"{daily['humidity'].min():.1f}-{daily['humidity'].max():.1f}%\"\n",
        "\n",
        "            # Soil\n",
        "            row3 = table.rows[3].cells\n",
        "            row3[0].text = \"üå± Soil Moisture\"\n",
        "            row3[1].text = f\"{daily['soil'].iloc[-1]:.1f}%\"\n",
        "            row3[2].text = f\"{daily['soil'].mean():.1f}%\"\n",
        "            row3[3].text = f\"{daily['soil'].min():.1f}-{daily['soil'].max():.1f}%\"\n",
        "\n",
        "        # Statistics Summary\n",
        "        doc.add_heading(\"Statistical Summary\", 1)\n",
        "        stats_text = f\"\"\"Total Readings: {len(df)}\n",
        "Time Period: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}\n",
        "Data Points: Temperature, Humidity, Soil Moisture\n",
        "Quality: All sensors operational\"\"\"\n",
        "        doc.add_paragraph(stats_text)\n",
        "\n",
        "        doc.save(output_path)\n",
        "        return output_path\n",
        "\n",
        "\n",
        "# Initialize report generator (same LM flow you already use)\n",
        "report_gen = AutomatedReportGenerator(client, REPORT_MODEL_NAME)\n",
        "\n",
        "\n",
        "def generate_report_screen(limit: int):\n",
        "    \"\"\"\n",
        "    Gradio button handler.\n",
        "    Returns: (status_text, file_path_or_None)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dfs = {\n",
        "            \"temperature\": load_iot_data(\"temperature\", limit),\n",
        "            \"humidity\": load_iot_data(\"humidity\", limit),\n",
        "            \"soil\": load_iot_data(\"soil\", limit),\n",
        "        }\n",
        "\n",
        "        df = unify_sensor_dfs(dfs)\n",
        "        df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "        if df.empty:\n",
        "            return \"No data available to generate a report.\", None\n",
        "\n",
        "\n",
        "        fd, path = tempfile.mkstemp(suffix=\".docx\", prefix=\"daily_report_\")\n",
        "        os.close(fd)\n",
        "\n",
        "        out_path = report_gen.create_docx_report(df, output_path=path)\n",
        "        return \"‚úÖ Report generated successfully. Download below:\", out_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating report: {str(e)}\", None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVEFwcIVNtzb"
      },
      "source": [
        "‚úÖ TAB 3 GUI - üìÑ Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ElkHtUANx4I"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# TAB 2 ‚Äî Generate Report (GUI ONLY)\n",
        "# =========================================================\n",
        "\n",
        "def build_generate_report_tab():\n",
        "    gr.Markdown(\"## üìÑ Generate Report\")\n",
        "    gr.Markdown(\"Generate a Word (DOCX) report based on sensor data: temperature, humidity, and soil moisture. (English AI summary)\")\n",
        "\n",
        "    report_samples = gr.Slider(\n",
        "        minimum=5,\n",
        "        maximum=200,\n",
        "        value=20,\n",
        "        step=1,\n",
        "        label=\"Number of samples per sensor\"\n",
        "    )\n",
        "\n",
        "    report_btn = gr.Button(\"üì• Generate & Download Report\", variant=\"primary\")\n",
        "    report_status = gr.Textbox(label=\"Status\", lines=2)\n",
        "    report_file = gr.File(label=\"Download DOCX\")\n",
        "\n",
        "    report_btn.click(\n",
        "        fn=generate_report_screen_gamified,   # ◊û◊í◊ô◊¢ ◊û◊î-LOGIC tab\n",
        "        inputs=[report_samples],\n",
        "        outputs=[report_status, report_file]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U06h9U6aiNy"
      },
      "source": [
        "TAB 4 LOGIC - üñºÔ∏è Plant Disease Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5KpoVVMbX8P"
      },
      "outputs": [],
      "source": [
        "def analyze_plant(image, temp, humidity, soil):\n",
        "    preds = clf(image)\n",
        "    top = preds[0]\n",
        "\n",
        "    label = top[\"label\"]\n",
        "    score = top[\"score\"]\n",
        "\n",
        "    alerts = []\n",
        "    advice = []\n",
        "\n",
        "    # --- Conditions based on sensors / user input ---\n",
        "    # Temperature: 18‚Äì32¬∞C\n",
        "    if temp < 18:\n",
        "      alerts.append(\"Low temperature\")\n",
        "      advice.append(\"Recommendation: move plant to a warmer environment\")\n",
        "\n",
        "    elif temp > 32:\n",
        "      alerts.append(\"High temperature\")\n",
        "      advice.append(\"Recommendation: move the plant to a shaded area\")\n",
        "\n",
        "    # Air Humidity: 35‚Äì75%\n",
        "    if humidity < 35:\n",
        "      alerts.append(\"Low air humidity\")\n",
        "      advice.append(\"Recommendation: increase humidity (e.g. misting)\")\n",
        "\n",
        "    elif humidity > 75:\n",
        "      alerts.append(\"High air humidity\")\n",
        "      advice.append(\"Recommendation: improve ventilation\")\n",
        "\n",
        "    # Soil Moisture: 20‚Äì60%\n",
        "    if soil < 20:\n",
        "      alerts.append(\"Low soil moisture\")\n",
        "      advice.append(\"Recommendation: irrigate / water the plant\")\n",
        "\n",
        "    elif soil > 60:\n",
        "      alerts.append(\"High soil moisture\")\n",
        "      advice.append(\"Recommendation: reduce watering\")\n",
        "\n",
        "\n",
        "    # --- Color status (\"flag\") based on image prediction ---\n",
        "    # Simple rule: if label contains \"healthy\" => good (green), else bad (red)\n",
        "    is_bad = (\"healthy\" not in label.lower())\n",
        "\n",
        "    status_html = (\n",
        "        \"<div style='padding:10px;border-radius:10px;\"\n",
        "        f\"background:{'#ffdddd' if is_bad else '#ddffdd'};\"\n",
        "        f\"border:1px solid {'#ff0000' if is_bad else '#00aa00'};\"\n",
        "        \"font-weight:700;'>\"\n",
        "        f\"{'üî¥ Plant status: BAD' if is_bad else 'üü¢ Plant status: GOOD'}\"\n",
        "        \"</div>\"\n",
        "    )\n",
        "\n",
        "    if not alerts:\n",
        "        alerts.append(\"Status looks normal\")\n",
        "\n",
        "    return (\n",
        "        f\"Detected disease: {label} ({score:.2%})\",\n",
        "        status_html,\n",
        "        \"\\n\".join(alerts),\n",
        "        \"\\n\".join(advice)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZTNbJe1amWj"
      },
      "source": [
        "TAB 4 GUI - üñºÔ∏è Plant Disease Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt5-ZORrbbt9"
      },
      "outputs": [],
      "source": [
        "def build_plant_disease_detection_tab():\n",
        "    gr.Markdown(\"## üñºÔ∏è Plant Disease Detection\")\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        # -------- LEFT SIDE --------\n",
        "        with gr.Column(scale=2):\n",
        "\n",
        "            image = gr.Image(\n",
        "                type=\"filepath\",\n",
        "                label=\"Upload plant image\",\n",
        "                sources=[\"upload\"]\n",
        "            )\n",
        "\n",
        "            temp = gr.Slider(0, 45, value=25, label=\"Temperature (¬∞C)\")\n",
        "            humidity = gr.Slider(0, 100, value=50, label=\"Humidity (%)\")\n",
        "            soil = gr.Slider(0, 100, value=50, label=\"Soil Moisture (%)\")\n",
        "\n",
        "            run_btn = gr.Button(\"Analyze Plant\", variant=\"primary\")\n",
        "\n",
        "        # -------- RIGHT SIDE --------\n",
        "        with gr.Column(scale=2):\n",
        "\n",
        "            diagnosis = gr.Textbox(\n",
        "                label=\"Diagnosis\",\n",
        "                placeholder=\"Plant disease diagnosis will appear here\"\n",
        "            )\n",
        "\n",
        "            status = gr.HTML(label=\"Status\")\n",
        "\n",
        "            alerts = gr.Textbox(\n",
        "                label=\"Alerts\",\n",
        "                lines=5,\n",
        "                placeholder=\"Sensor alerts and warnings will appear here\"\n",
        "            )\n",
        "\n",
        "            recommendations = gr.Textbox(\n",
        "                label=\"Recommendations\",\n",
        "                lines=5,\n",
        "                placeholder=\"Care and treatment recommendations will appear here\"\n",
        "            )\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=analyze_plant_gamified,\n",
        "        inputs=[image, temp, humidity, soil],\n",
        "        outputs=[diagnosis, status, alerts, recommendations]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkSvfBItbI0T"
      },
      "source": [
        "TAB 5 LOGIC - RAG Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xg6U8P7bHTm"
      },
      "outputs": [],
      "source": [
        "DOC_URLS = [\n",
        "    \"https://doi.org/10.1038/s41598-025-20629-y\",\n",
        "    \"https://doi.org/10.3389/fpls.2016.01419\",\n",
        "    \"https://doi.org/10.1038/s41598-025-05102-0\",\n",
        "    \"https://doi.org/10.1038/s41598-025-04758-y\",\n",
        "    \"https://doi.org/10.2174/0118743315321139240627092707\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWKeK8JEcfJL"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 2: HTTP / Session setup\n",
        "# =========================\n",
        "# Purpose: Shared HTTP session + helper functions to fetch HTML/PDF and query academic APIs (Semantic Scholar/OpenAlex/Unpaywall).\n",
        "\n",
        "\n",
        "\n",
        "# Reuse a single session for performance and consistent headers/cookies.\n",
        "session = requests.Session()\n",
        "\n",
        "# Browser-like headers to improve compatibility with some sites.\n",
        "BROWSER_HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "# Normalize DOI input (accept DOI or doi.org URL).\n",
        "def _normalize_doi(doi_or_url: str) -> str:\n",
        "    s = (doi_or_url or \"\").strip()\n",
        "    s = s.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n",
        "    return s.strip()\n",
        "\n",
        "# ---------- Added: text quality post-processing helpers ----------\n",
        "\n",
        "_STOP_SECTION_TITLES = {\n",
        "    \"references\", \"reference\", \"bibliography\",\n",
        "    \"acknowledgements\", \"acknowledgments\",\n",
        "    \"author information\", \"ethics declarations\",\n",
        "    \"additional information\", \"supplementary information\",\n",
        "    \"rights and permissions\", \"about this article\",\n",
        "    \"availability of data and materials\", \"data availability\",\n",
        "    \"publisher's note\"\n",
        "}\n",
        "\n",
        "_SKIP_LINE_RES = [\n",
        "    re.compile(r\"(?i)^\\s*cite this article\\s*$\"),\n",
        "    re.compile(r\"(?i)^\\s*article\\s+google\\s+scholar\\s*$\"),\n",
        "    re.compile(r\"(?i)^\\s*google\\s+scholar\\s*$\"),\n",
        "    re.compile(r\"(?i)^\\s*pubmed\\s*$\"),\n",
        "    re.compile(r\"(?i)^\\s*pubmed\\s+central\\s*$\"),\n",
        "    re.compile(r\"(?i)^\\s*ads\\s*$\"),\n",
        "    re.compile(r\"(?i)^\\s*cas\\s*$\"),\n",
        "    re.compile(r\"(?i)creative\\s+commons\"),\n",
        "    re.compile(r\"(?i)springer\\s+nature\"),\n",
        "    re.compile(r\"(?i)sharedit\"),\n",
        "    re.compile(r\"(?i)correspondence\\s+to\"),\n",
        "    re.compile(r\"(?i)the\\s+authors\\s+declare\\s+no\\s+competing\\s+interests\"),\n",
        "    re.compile(r\"(?i)view\\s+author\\s+publications\"),\n",
        "    re.compile(r\"(?i)search\\s+author\\s+on\"),\n",
        "    re.compile(r\"(?i)anyone\\s+you\\s+share\\s+the\\s+following\\s+link\"),\n",
        "    re.compile(r\"(?i)version\\s+of\\s+record\"),\n",
        "    re.compile(r\"(?i)copyright\"),\n",
        "    re.compile(r\"(?i)rights\\s+reserved\"),\n",
        "]\n",
        "\n",
        "def normalize_text_keep_newlines(text: str) -> str:\n",
        "    \"\"\"Normalize whitespace while preserving newlines.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    # Collapse spaces/tabs inside lines\n",
        "    text = \"\\n\".join(re.sub(r\"[ \\t]+\", \" \", ln).strip() for ln in text.split(\"\\n\"))\n",
        "    # Collapse too many blank lines\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
        "    return text\n",
        "\n",
        "def looks_like_blocked_or_boilerplate(text: str) -> bool:\n",
        "    \"\"\"Detect common anti-bot/paywall/cookie boilerplate that inflates text.\"\"\"\n",
        "    s = (text or \"\").strip().lower()\n",
        "    if not s:\n",
        "        return False\n",
        "    bad_phrases = [\n",
        "        \"verify you are human\",\n",
        "        \"enable javascript\",\n",
        "        \"turn on javascript\",\n",
        "        \"access denied\",\n",
        "        \"unusual traffic\",\n",
        "        \"cookie\",\n",
        "        \"cookies\",\n",
        "        \"privacy policy\",\n",
        "        \"terms of use\",\n",
        "        \"we value your privacy\",\n",
        "        \"sign in to continue\",\n",
        "        \"subscribe to\",\n",
        "        \"accept all cookies\",\n",
        "        \"manage cookies\",\n",
        "    ]\n",
        "    return any(p in s for p in bad_phrases)\n",
        "\n",
        "def _should_skip_line(line: str) -> bool:\n",
        "    s = (line or \"\").strip()\n",
        "    if not s:\n",
        "        return False\n",
        "    if len(s) <= 3:\n",
        "        return True\n",
        "    for rx in _SKIP_LINE_RES:\n",
        "        if rx.search(s):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def postprocess_document_text(text: str, max_chars: int = 50000) -> str:\n",
        "    \"\"\"\n",
        "    Normalize + remove boilerplate + cut references + de-duplicate paragraphs.\n",
        "    Keeps 'TITLE:' / 'ABSTRACT:' markers if present.\n",
        "    \"\"\"\n",
        "    text = normalize_text_keep_newlines(text or \"\")\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove noisy single lines\n",
        "    lines = []\n",
        "    for ln in text.splitlines():\n",
        "        if _should_skip_line(ln):\n",
        "            continue\n",
        "        if looks_like_blocked_or_boilerplate(ln):\n",
        "            continue\n",
        "        lines.append(ln.rstrip())\n",
        "    text = \"\\n\".join(lines).strip()\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Cut everything after an explicit references/bibliography header line\n",
        "    m = re.search(r\"(?im)^\\s*(references|bibliography)\\s*$\", text)\n",
        "    if m:\n",
        "        text = text[:m.start()].rstrip()\n",
        "\n",
        "    # Keep only the first TITLE: line if repeated\n",
        "    out_lines = []\n",
        "    title_seen = False\n",
        "    for ln in text.splitlines():\n",
        "        if ln.startswith(\"TITLE:\"):\n",
        "            if title_seen:\n",
        "                continue\n",
        "            title_seen = True\n",
        "        out_lines.append(ln)\n",
        "    text = \"\\n\".join(out_lines).strip()\n",
        "\n",
        "    # Paragraph de-duplication (removes duplicates anywhere, not only consecutive)\n",
        "    paras = [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip()]\n",
        "    seen = set()\n",
        "    deduped = []\n",
        "    for p in paras:\n",
        "        key = re.sub(r\"\\W+\", \"\", p.lower())[:900]\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        deduped.append(p)\n",
        "\n",
        "    text = \"\\n\\n\".join(deduped).strip()\n",
        "\n",
        "    # Hard cap (avoid storing extremely large junk)\n",
        "    if len(text) > max_chars:\n",
        "        text = text[:max_chars].rstrip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Fetch HTML and return (html_text, final_url, status_code).\n",
        "def fetch_html(url: str, timeout: int = 25):\n",
        "    r = session.get(\n",
        "        url,\n",
        "        headers={**BROWSER_HEADERS, \"Accept\": \"text/html,*/*;q=0.8\"},\n",
        "        timeout=timeout,\n",
        "        allow_redirects=True\n",
        "    )\n",
        "    return (r.text or \"\"), r.url, r.status_code\n",
        "\n",
        "# Extract readable main text from HTML (title/description/body).\n",
        "def extract_main_text_from_html(html: str) -> str:\n",
        "    try:\n",
        "        from bs4 import BeautifulSoup\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Install bs4 + lxml: pip install beautifulsoup4 lxml\")\n",
        "\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Remove obvious non-content areas\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"iframe\"]):\n",
        "        tag.decompose()\n",
        "    for tag in soup.find_all([\"header\", \"footer\", \"nav\", \"aside\", \"form\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    root = soup.find(\"main\") or soup.find(\"article\") or soup\n",
        "\n",
        "    title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
        "    desc = \"\"\n",
        "    m = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "    if m and m.get(\"content\"):\n",
        "        desc = m[\"content\"].strip()\n",
        "\n",
        "    chunks = []\n",
        "    for el in root.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n",
        "        t = el.get_text(\" \", strip=True)\n",
        "\n",
        "        # Stop when reaching tail sections (References, etc.)\n",
        "        if el.name in [\"h1\", \"h2\", \"h3\"]:\n",
        "            if (t or \"\").strip().lower() in _STOP_SECTION_TITLES:\n",
        "                break\n",
        "\n",
        "        if not t or len(t) < 30:\n",
        "            continue\n",
        "        if looks_like_blocked_or_boilerplate(t):\n",
        "            continue\n",
        "        if _should_skip_line(t):\n",
        "            continue\n",
        "\n",
        "        chunks.append(t)\n",
        "\n",
        "    # Fallback if structure is weak\n",
        "    if len(chunks) < 3:\n",
        "        t = re.sub(r\"\\s+\", \" \", root.get_text(\" \", strip=True)).strip()\n",
        "        chunks = [t] if t else []\n",
        "\n",
        "    # De-duplicate keeping order\n",
        "    chunks = list(dict.fromkeys(chunks))\n",
        "\n",
        "    # IMPORTANT: keep paragraph boundaries so paragraph de-dup works later\n",
        "    body = \"\\n\\n\".join(chunks).strip()\n",
        "\n",
        "    # Drop DESCRIPTION if it's basically contained in the body\n",
        "    if desc:\n",
        "        norm_desc = re.sub(r\"\\W+\", \"\", desc.lower())\n",
        "        norm_body_head = re.sub(r\"\\W+\", \"\", body.lower()[: max(2000, len(desc) * 3)])\n",
        "        if norm_desc and norm_desc in norm_body_head:\n",
        "            desc = \"\"\n",
        "\n",
        "    parts = []\n",
        "    if title:\n",
        "        parts.append(f\"TITLE: {title}\")\n",
        "    if desc:\n",
        "        parts.append(f\"DESCRIPTION: {desc}\")\n",
        "    if body:\n",
        "        parts.append(body)\n",
        "\n",
        "    return postprocess_document_text(\"\\n\\n\".join(parts).strip())\n",
        "\n",
        "\n",
        "# API lookup: Semantic Scholar (metadata + possible openAccessPdf).\n",
        "def semantic_scholar_lookup(doi: str):\n",
        "    \"\"\"Return dict with title/abstract and possibly openAccessPdf url.\"\"\"\n",
        "    # No key required for basic use, but rate-limited.\n",
        "    url = f\"https://api.semanticscholar.org/graph/v1/paper/DOI:{quote(doi, safe='')}\"\n",
        "    params = {\"fields\": \"title,abstract,openAccessPdf,url\"}\n",
        "    r = session.get(url, params=params, headers=BROWSER_HEADERS, timeout=25)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "# API lookup: OpenAlex (metadata + locations + inverted-index abstract).\n",
        "def openalex_lookup(doi: str):\n",
        "    url = f\"https://api.openalex.org/works/doi:{quote(doi, safe='')}\"\n",
        "    r = session.get(url, headers=BROWSER_HEADERS, timeout=25)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "# API lookup: Unpaywall (OA locations; requires email parameter).\n",
        "def unpaywall_lookup(doi: str, email: str = \"test@example.com\"):\n",
        "    # Unpaywall requires an email parameter (your real email is best).\n",
        "    url = f\"https://api.unpaywall.org/v2/{quote(doi, safe='')}\"\n",
        "    r = session.get(url, params={\"email\": email}, headers=BROWSER_HEADERS, timeout=25)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "# Select the best PDF URL from available sources (Semantic Scholar/OpenAlex/Unpaywall).\n",
        "def _pick_pdf_url_from_sources(ss=None, oa=None, up=None) -> str:\n",
        "    # 1) Semantic Scholar openAccessPdf\n",
        "    if ss and isinstance(ss, dict):\n",
        "        oap = ss.get(\"openAccessPdf\") or {}\n",
        "        if isinstance(oap, dict) and oap.get(\"url\"):\n",
        "            return oap[\"url\"]\n",
        "\n",
        "    # 2) OpenAlex primary_location / open_access\n",
        "    if oa and isinstance(oa, dict):\n",
        "        pl = oa.get(\"primary_location\") or {}\n",
        "        if isinstance(pl, dict):\n",
        "            pdf = pl.get(\"pdf_url\")\n",
        "            if pdf:\n",
        "                return pdf\n",
        "            landing = pl.get(\"landing_page_url\")\n",
        "            if landing and landing.lower().endswith(\".pdf\"):\n",
        "                return landing\n",
        "        oa2 = oa.get(\"open_access\") or {}\n",
        "        if isinstance(oa2, dict):\n",
        "            oa_url = oa2.get(\"oa_url\")\n",
        "            if oa_url and oa_url.lower().endswith(\".pdf\"):\n",
        "                return oa_url\n",
        "\n",
        "    # 3) Unpaywall best_oa_location\n",
        "    if up and isinstance(up, dict):\n",
        "        bol = up.get(\"best_oa_location\") or {}\n",
        "        if isinstance(bol, dict):\n",
        "            pdf = bol.get(\"url_for_pdf\")\n",
        "            if pdf:\n",
        "                return pdf\n",
        "            url = bol.get(\"url\")\n",
        "            if url and url.lower().endswith(\".pdf\"):\n",
        "                return url\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Download a PDF and extract text from the first pages (bounded by max_pages).\n",
        "def extract_text_from_pdf_url(pdf_url: str, max_pages: int = 8) -> str:\n",
        "    \"\"\"Download PDF and extract text from first max_pages pages.\"\"\"\n",
        "    r = session.get(\n",
        "        pdf_url,\n",
        "        headers={**BROWSER_HEADERS, \"Accept\": \"application/pdf,*/*;q=0.8\"},\n",
        "        timeout=40,\n",
        "        allow_redirects=True\n",
        "    )\n",
        "    if r.status_code != 200 or not r.content:\n",
        "        return \"\"\n",
        "\n",
        "    data = r.content\n",
        "\n",
        "    # pypdf\n",
        "    try:\n",
        "        from pypdf import PdfReader\n",
        "        import io\n",
        "        reader = PdfReader(io.BytesIO(data))\n",
        "        out = []\n",
        "        n = min(len(reader.pages), max_pages)\n",
        "        for i in range(n):\n",
        "            out.append(reader.pages[i].extract_text() or \"\")\n",
        "        text = \"\\n\".join(out).strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        return text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # pdfminer\n",
        "    try:\n",
        "        from pdfminer.high_level import extract_text\n",
        "        import io\n",
        "        text = extract_text(io.BytesIO(data), maxpages=max_pages) or \"\"\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# Utility: reconstruct OpenAlex inverted-index abstract into ordered text.\n",
        "def _reconstruct_abstract_from_inverted_index(inv: dict) -> str:\n",
        "    \"\"\"\n",
        "    OpenAlex returns abstracts as an inverted index:\n",
        "    {word: [pos1, pos2, ...], ...}\n",
        "    This reconstructs the abstract text in the correct order.\n",
        "    \"\"\"\n",
        "    if not isinstance(inv, dict) or not inv:\n",
        "        return \"\"\n",
        "\n",
        "    # Find max position to size the token list\n",
        "    max_pos = -1\n",
        "    for positions in inv.values():\n",
        "        if isinstance(positions, list) and positions:\n",
        "            mp = max(positions)\n",
        "            if mp > max_pos:\n",
        "                max_pos = mp\n",
        "\n",
        "    if max_pos < 0:\n",
        "        return \"\"\n",
        "\n",
        "    tokens = [\"\"] * (max_pos + 1)\n",
        "\n",
        "    for word, positions in inv.items():\n",
        "        if not isinstance(positions, list):\n",
        "            continue\n",
        "        for p in positions:\n",
        "            if isinstance(p, int) and 0 <= p < len(tokens):\n",
        "                tokens[p] = word\n",
        "\n",
        "    # Join; remove empties\n",
        "    text = \" \".join(t for t in tokens if t)\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtNn87uDcpAW"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3. DOI Metadata fallback (only if HTML is short)\n",
        "# =========================\n",
        "# Purpose: Fetch usable document text from a URL with fallbacks:\n",
        "# HTML -> Open-Access PDF (via APIs) -> Abstract metadata -> Title-only.\n",
        "\n",
        "def get_document_text(url: str, min_chars: int = 800, unpaywall_email: str = \"test@example.com\", debug: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Returns the best available text we can legally obtain.\n",
        "    Order:\n",
        "      1) HTML extraction (if enough)\n",
        "      2) OA PDF extraction via Semantic Scholar / OpenAlex / Unpaywall (if available)\n",
        "      3) Abstract via Semantic Scholar/OpenAlex (fallback)\n",
        "      4) Title only\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Try extracting text directly from the landing HTML page.\n",
        "    html, final_url, status = fetch_html(url)\n",
        "    text_html_raw = extract_main_text_from_html(html) if html else \"\"\n",
        "    text_html = postprocess_document_text(text_html_raw)\n",
        "\n",
        "    # Optional debug printing for tracing which path is used.\n",
        "    if debug:\n",
        "        print(\"final_url:\", final_url)\n",
        "        print(\"html_status:\", status, \"| html_text_chars_raw:\", len(text_html_raw), \"| html_text_chars_clean:\", len(text_html))\n",
        "\n",
        "    # If cleaned HTML content is sufficient, return it immediately.\n",
        "    if len(text_html) >= min_chars:\n",
        "        return text_html\n",
        "\n",
        "    # 2) If input is a DOI link or a DOI string, try Open-Access routes (PDF).\n",
        "    u = (url or \"\").strip()\n",
        "    doi = \"\"\n",
        "    if \"doi.org/\" in u:\n",
        "        doi = _normalize_doi(u)\n",
        "    elif re.match(r\"^10\\.\\d{4,9}/\\S+$\", u):\n",
        "        doi = _normalize_doi(u)\n",
        "\n",
        "    ss = oa = up = None\n",
        "\n",
        "    if doi:\n",
        "        # Query multiple services to maximize chances of finding an OA PDF.\n",
        "        try:\n",
        "            ss = semantic_scholar_lookup(doi)\n",
        "        except Exception:\n",
        "            ss = None\n",
        "        try:\n",
        "            oa = openalex_lookup(doi)\n",
        "        except Exception:\n",
        "            oa = None\n",
        "        try:\n",
        "            up = unpaywall_lookup(doi, email=unpaywall_email)\n",
        "        except Exception:\n",
        "            up = None\n",
        "\n",
        "        # Pick the best PDF URL from the available sources (if any).\n",
        "        pdf_url = _pick_pdf_url_from_sources(ss=ss, oa=oa, up=up)\n",
        "        if debug:\n",
        "            print(\"pdf_url:\", pdf_url or \"(none)\")\n",
        "\n",
        "        # If we found a PDF, extract text from the first pages.\n",
        "        if pdf_url:\n",
        "            pdf_text_raw = extract_text_from_pdf_url(pdf_url, max_pages=8)\n",
        "            # Optionally prepend a title header when available.\n",
        "            title = (ss or {}).get(\"title\") or (oa or {}).get(\"title\") or \"\"\n",
        "            title = title.get(\"display_name\") if isinstance(title, dict) else title\n",
        "            header = f\"TITLE: {title}\".strip() if title else \"\"\n",
        "\n",
        "            merged = (header + \"\\n\" + pdf_text_raw).strip() if header else (pdf_text_raw or \"\").strip()\n",
        "            pdf_text = postprocess_document_text(merged)\n",
        "\n",
        "            if debug:\n",
        "                print(\"pdf_text_chars_raw:\", len(pdf_text_raw), \"| pdf_text_chars_clean:\", len(pdf_text))\n",
        "\n",
        "            if len(pdf_text) >= min_chars:\n",
        "                return pdf_text\n",
        "\n",
        "        # 3) If PDF is missing/short, fall back to abstract metadata (when present).\n",
        "        title = \"\"\n",
        "        abstract = \"\"\n",
        "\n",
        "        if ss and isinstance(ss, dict):\n",
        "            title = (ss.get(\"title\") or \"\").strip()\n",
        "            abstract = (ss.get(\"abstract\") or \"\").strip()\n",
        "\n",
        "        if (not abstract) and oa and isinstance(oa, dict):\n",
        "            t = oa.get(\"title\") or \"\"\n",
        "            title = title or t\n",
        "            # OpenAlex abstract can be stored as an inverted index.\n",
        "            inv = oa.get(\"abstract_inverted_index\")\n",
        "            if inv:\n",
        "                abstract = _reconstruct_abstract_from_inverted_index(inv)\n",
        "\n",
        "        # Return any metadata we managed to obtain.\n",
        "        if title or abstract:\n",
        "            parts = []\n",
        "            if title:\n",
        "                parts.append(f\"TITLE: {title}\")\n",
        "            if abstract:\n",
        "                parts.append(f\"ABSTRACT: {abstract}\")\n",
        "            return postprocess_document_text(\"\\n\".join(parts).strip())\n",
        "\n",
        "    # 4) Last resort: return thin HTML (title/description) if present.\n",
        "    if text_html:\n",
        "        return postprocess_document_text(text_html.strip())\n",
        "\n",
        "    # Final fallback: return the URL as a minimal title marker.\n",
        "    return postprocess_document_text(f\"TITLE: {url}\".strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN9YEitTctBm"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4. NLP Preprocessing\n",
        "# =========================\n",
        "# Purpose: Standardize text into comparable tokens for indexing/search.\n",
        "# Pipeline: tokenize -> stopword removal -> stemming.\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Convert text to a list of lowercase word tokens.\"\"\"\n",
        "    return re.findall(r\"\\w+\", (text or \"\").lower())\n",
        "\n",
        "def remove_stopwords(tokens, stop_words):\n",
        "    \"\"\"Remove stop words from a list of tokens.\"\"\"\n",
        "    return [t for t in tokens if t not in stop_words]\n",
        "\n",
        "def apply_stemming(tokens):\n",
        "    \"\"\"Apply Porter stemming to a list of tokens.\"\"\"\n",
        "    return [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "\n",
        "def preprocess_query(query: str):\n",
        "    # Goal: Apply the same normalization steps used for documents to the user query.\n",
        "    tokens = tokenize(query)\n",
        "    tokens = remove_stopwords(tokens, stop_words)\n",
        "    tokens = apply_stemming(tokens)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnI_A-yScy4j"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 5. Document Text Access\n",
        "# =========================\n",
        "# Purpose: Fetch and store raw text for each document URL so it can be indexed later.\n",
        "# Output: doc_text dict mapping doc_id -> extracted text (may be empty on failure).\n",
        "\n",
        "def build_doc_text_map(doc_urls, min_chars=800, unpaywall_email=\"test@example.com\", debug=False):\n",
        "    doc_text = {}\n",
        "    for i, url in enumerate(doc_urls):\n",
        "        # Try to extract the best available text (HTML/PDF/abstract fallback).\n",
        "        try:\n",
        "            doc_text[i] = get_document_text(\n",
        "                url,\n",
        "                min_chars=min_chars,\n",
        "                unpaywall_email=unpaywall_email,\n",
        "                debug=debug\n",
        "            ) or \"\"\n",
        "        except Exception as e:\n",
        "            # Keep the pipeline running even if a document fails to fetch.\n",
        "            print(f\"Failed to fetch document {i}: {e}\")\n",
        "            doc_text[i] = \"\"\n",
        "    return doc_text\n",
        "\n",
        "\n",
        "# Placeholder / initialization for the document-text mapping (doc_id -> text).\n",
        "doc_text = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQYDi7e4c0oG"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6. Index Construction\n",
        "# =========================\n",
        "# Purpose: Build an inverted index for fast keyword-based retrieval.\n",
        "# - Input: raw document texts (doc_text) and preprocessing settings (stop_words).\n",
        "# - Output:\n",
        "#   1) inverted: term -> list of doc_ids containing the term\n",
        "#   2) doc_map: doc_id -> original URL\n",
        "\n",
        "def build_inverted_index(urls, stop_words, doc_text):\n",
        "    inverted = defaultdict(set)  # term -> set(doc_ids)\n",
        "    doc_map = {i: url for i, url in enumerate(urls)}  # DocID -> URL\n",
        "\n",
        "    for doc_id in range(len(urls)):\n",
        "        # Get the stored text for this document (support int or string keys).\n",
        "        text = doc_text.get(doc_id) or doc_text.get(str(doc_id)) or \"\"\n",
        "\n",
        "        # Normalize document text into searchable terms.\n",
        "        tokens = tokenize(text)\n",
        "        tokens = remove_stopwords(tokens, stop_words)\n",
        "        tokens = apply_stemming(tokens)\n",
        "\n",
        "        # Add each unique term to the index for this document.\n",
        "        for term in set(tokens):\n",
        "            inverted[term].add(doc_id)\n",
        "\n",
        "    # Convert sets to sorted lists for stable output/serialization.\n",
        "    inverted = {term: sorted(list(ids)) for term, ids in inverted.items()}\n",
        "    return inverted, doc_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpwLmT6Fc9BT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "1c0f95ebb72b4e6f93c56e9e9a4a2ea8",
            "9f949420883b49cd9803752cc9d71a95",
            "a93a6a45526748ea9b64301646d06010",
            "0d330e0c46e545fa84c689ccd19e3632",
            "ae8caf58b86a41348baed61f32fdfb90",
            "5818ef8eef3342b18a312d1c6e30b683",
            "1f47d9634f584206a41a064251147fb0",
            "a598ae3615594b9ba8f9c4d67220bb6f",
            "3085076ff89d4f32b2393875d89ce1fd",
            "fec8d79926064dc0a02328177404721f",
            "93c47a84e19d42eb9daf81775c1dba66"
          ]
        },
        "outputId": "9f74424f-f2e4-4787-fa02-92b77cf4defe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c0f95ebb72b4e6f93c56e9e9a4a2ea8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GET indexes/public_index status: 200 | resp: {\"0\":[0,1,2,3],\"00\":[2],\"000\":[0],\"1\":[0,1,2,3],\"01\":[0],\"0001\":[2,3],\"2\":[0,1,2,3,4],\"3\":[0,1,2,3],\"4\":[0,1,2,3],\"04\":[2],\"5\":[0,1,2,3],\"05\":[3],\"6\":[0,1,2,3],\"7\":[0,1,2,3],\"07\":[0],\"8\":[0,1,2,3],\"9\"\n",
            "GET indexes/doc_map status: 200 | resp: [\"https://doi.org/10.1038/s41598-025-20629-y\",\"https://doi.org/10.3389/fpls.2016.01419\",\"https://doi.org/10.1038/s41598-025-05102-0\",\"https://doi.org/10.1038/s41598-025-04758-y\",\"https://doi.org/10.21\n",
            "GET indexes/doc_text status: 200 | resp: [\"TITLE: Medicinal plant leaf disease classification using optimal weighted features with dilated adaptive DenseNet and attention mechanism | Scientific Reports\\n\\nThe agriculture sector plays a pivot\n",
            "‚úÖ Loaded existing store from Firebase (no rebuild).\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 7. Firebase I/O\n",
        "# =========================\n",
        "# Purpose: Persist and reuse the document store in Firebase.\n",
        "# Stored objects:\n",
        "# - doc_text: doc_id -> extracted text\n",
        "# - public_index: inverted index (term -> doc_ids)\n",
        "# - doc_map: doc_id -> source URL\n",
        "\n",
        "def save_to_firebase(data, path):\n",
        "    # Write JSON data to a Firebase Realtime Database path using HTTP PUT.\n",
        "    base = FIREBASE_URL.rstrip(\"/\")\n",
        "    url = f\"{base}/{path}.json\"\n",
        "    r = requests.put(url, json=data, timeout=30)\n",
        "    print(\"PUT\", path, \"status:\", r.status_code, \"| resp:\", r.text[:200])\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"PUT {path} failed: {r.status_code} | {r.text[:400]}\")\n",
        "    return r.status_code, r.text\n",
        "\n",
        "\n",
        "def firebase_get(path):\n",
        "    # Read JSON data from a Firebase Realtime Database path using HTTP GET.\n",
        "    base = FIREBASE_URL.rstrip(\"/\")\n",
        "    url = f\"{base}/{path}.json\"\n",
        "    r = requests.get(url, timeout=30)\n",
        "    print(\"GET\", path, \"status:\", r.status_code, \"| resp:\", r.text[:200])\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"GET {path} failed: {r.status_code} | {r.text[:400]}\")\n",
        "    return r.json()\n",
        "\n",
        "\n",
        "def build_and_save_index(\n",
        "    urls,\n",
        "    stop_words,\n",
        "    index_path=\"indexes/public_index\",\n",
        "    map_path=\"indexes/doc_map\",\n",
        "    text_path=\"indexes/doc_text\"):\n",
        "    # Orchestrator: build doc_text + inverted index locally, then save all artifacts to Firebase.\n",
        "\n",
        "    # 1) Build doc_text locally\n",
        "    doc_text_local = build_doc_text_map(urls)\n",
        "\n",
        "    # Save doc_text with string keys (JSON consistency)\n",
        "    doc_text_to_save = {str(k): v for k, v in doc_text_local.items()}\n",
        "    save_to_firebase(doc_text_to_save, text_path)\n",
        "    print(\"‚úÖ doc_text saved\")\n",
        "\n",
        "    # 2) Build index + doc_map using doc_text_local\n",
        "    inv_index, doc_map = build_inverted_index(urls, stop_words, doc_text_local)\n",
        "\n",
        "    # Save doc_map with string keys\n",
        "    doc_map_json = {str(k): v for k, v in doc_map.items()}\n",
        "    save_to_firebase(inv_index, index_path)\n",
        "    print(\"‚úÖ index saved\")\n",
        "\n",
        "    save_to_firebase(doc_map_json, map_path)\n",
        "    print(\"‚úÖ doc_map saved\")\n",
        "\n",
        "    return inv_index, doc_map_json, doc_text_local\n",
        "\n",
        "# Load existing store from Firebase if present; otherwise build once and save.\n",
        "existing_index = firebase_get(\"indexes/public_index\")  # None if missing\n",
        "existing_map   = firebase_get(\"indexes/doc_map\")\n",
        "existing_text  = firebase_get(\"indexes/doc_text\")\n",
        "\n",
        "if existing_index is not None and existing_map is not None and existing_text is not None:\n",
        "    # Reuse cached data to avoid rebuilding/re-fetching documents.\n",
        "    inv_index = existing_index\n",
        "    doc_map_json = existing_map\n",
        "    doc_text_local = existing_text\n",
        "    print(\"‚úÖ Loaded existing store from Firebase (no rebuild).\")\n",
        "else:\n",
        "    # First-time setup: build and persist the store.\n",
        "    inv_index, doc_map_json, doc_text_local = build_and_save_index(DOC_URLS, stop_words)\n",
        "    print(\"‚úÖ Built and saved store to Firebase.\")\n",
        "\n",
        "# Expose globals used by retrieval/search code.\n",
        "public_index = inv_index\n",
        "doc_map = doc_map_json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymnru7rrdERK"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 8 Load store from Firebase\n",
        "# =========================\n",
        "# Purpose: Load previously saved index artifacts from Firebase into runtime globals.\n",
        "# Loads: public_index (inverted index), doc_map (doc_id -> URL), and optionally doc_text.\n",
        "\n",
        "INDEX_PATH = \"indexes/public_index\"\n",
        "MAP_PATH   = \"indexes/doc_map\"\n",
        "TEXT_PATH  = \"indexes/doc_text\"\n",
        "\n",
        "public_index = None\n",
        "doc_map = None\n",
        "doc_text = None\n",
        "\n",
        "def load_store_from_firebase(load_text: bool = False):\n",
        "    # Load index + doc_map from Firebase into globals\n",
        "    global public_index, doc_map, doc_text\n",
        "\n",
        "    # Always load index and doc_map (use empty dict if missing).\n",
        "    public_index = firebase_get(INDEX_PATH) or {}\n",
        "    doc_map = firebase_get(MAP_PATH) or {}\n",
        "\n",
        "    # Normalize doc_map shape if Firebase returns a JSON array instead of an object.\n",
        "    if isinstance(doc_map, list):\n",
        "      doc_map = {str(i): v for i, v in enumerate(doc_map)}\n",
        "\n",
        "    # Optionally load the full document texts (can be large).\n",
        "    if load_text:\n",
        "        doc_text = firebase_get(TEXT_PATH) or {}\n",
        "\n",
        "    # Basic summary for quick sanity-check.\n",
        "    print(\"Loaded:\",\n",
        "          \"terms=\", len(public_index),\n",
        "          \"| docs=\", len(doc_map) if hasattr(doc_map, \"__len__\") else type(doc_map))\n",
        "\n",
        "    return public_index, doc_map, doc_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRF_hY27dKdi"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 9. Retrieval (uses data loaded from Firebase)\n",
        "# =========================\n",
        "# Purpose: Perform simple keyword-based retrieval using the inverted index.\n",
        "# Method: preprocess query -> count matching terms per document -> return top-k docs.\n",
        "\n",
        "def search_top_k(query: str, k: int = 3):\n",
        "    # Ensure index and doc_map are available in memory (load if missing).\n",
        "    if public_index is None or doc_map is None:\n",
        "        load_store_from_firebase(load_text=False)\n",
        "\n",
        "    # Convert the raw query into normalized terms (tokenize/stopwords/stemming).\n",
        "    q_terms = preprocess_query(query)\n",
        "    scores = defaultdict(int)\n",
        "\n",
        "    # Score documents by how many query terms they contain.\n",
        "    for term in q_terms:\n",
        "        for doc_id in (public_index.get(term, []) or []):\n",
        "            scores[int(doc_id)] += 1\n",
        "\n",
        "    # Rank by score and keep the top-k.\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "\n",
        "    # Build a compact result list with doc id, score, and source URL.\n",
        "    results = []\n",
        "    for doc_id, score in ranked:\n",
        "        url = doc_map.get(str(doc_id)) if isinstance(doc_map, dict) else None\n",
        "        results.append({\"doc_id\": doc_id, \"score\": score, \"url\": url})\n",
        "\n",
        "    return q_terms, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2o5n47cdLRO"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 10. Ranking (BM25) + Gemini LLM bridge\n",
        "# =========================\n",
        "# Assumes these already exist in your notebook (do NOT redefine them here):\n",
        "# - public_index (dict: term -> [doc_ids])  [optional for fallback scoring]\n",
        "# - doc_map (list or dict: doc_id -> source_url)\n",
        "# - preprocess_query(text_or_query) -> List[str]\n",
        "# - firebase_get(path: str) -> Any\n",
        "# - load_store_from_firebase(load_text: bool = False)\n",
        "\n",
        "_DOC_TEXT_CACHE: Dict[int, str] = {}\n",
        "_DOC_TF_CACHE: Dict[int, Counter] = {}\n",
        "_DOC_LEN_CACHE: Dict[int, int] = {}\n",
        "_BM25_STATS_CACHE: Dict[str, Any] = {}\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "\n",
        "def _safe_str(x: Any) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    return str(x)\n",
        "\n",
        "def _ensure_stores_loaded() -> None:\n",
        "    if globals().get(\"doc_map\") is None or globals().get(\"public_index\") is None:\n",
        "        load_store_from_firebase(load_text=False)\n",
        "\n",
        "def _all_doc_ids() -> List[int]:\n",
        "    _ensure_stores_loaded()\n",
        "    dm = globals().get(\"doc_map\")\n",
        "\n",
        "    if isinstance(dm, list):\n",
        "        return list(range(len(dm)))\n",
        "\n",
        "    if isinstance(dm, dict):\n",
        "        ids = []\n",
        "        for k in dm.keys():\n",
        "            try:\n",
        "                ids.append(int(k))\n",
        "            except Exception:\n",
        "                continue\n",
        "        return sorted(set(ids))\n",
        "\n",
        "    return []\n",
        "\n",
        "def _get_source_url(doc_id: int) -> str:\n",
        "    dm = globals().get(\"doc_map\")\n",
        "    if isinstance(dm, list):\n",
        "        if 0 <= doc_id < len(dm):\n",
        "            return _safe_str(dm[doc_id]).strip()\n",
        "        return \"\"\n",
        "    if isinstance(dm, dict):\n",
        "        u = dm.get(doc_id)\n",
        "        if u is None:\n",
        "            u = dm.get(str(doc_id))\n",
        "        return _safe_str(u).strip()\n",
        "    return \"\"\n",
        "\n",
        "def get_doc_text(doc_id: int) -> str:\n",
        "    if doc_id in _DOC_TEXT_CACHE:\n",
        "        return _DOC_TEXT_CACHE[doc_id] or \"\"\n",
        "\n",
        "    key = str(int(doc_id))\n",
        "\n",
        "    base_candidates = [\n",
        "        \"indexes/doc_text\",\n",
        "        \"indexes/docTexts\",\n",
        "        \"indexes/documents_text\",\n",
        "        \"indexes/docs_text\",\n",
        "    ]\n",
        "    meta_candidates = [\n",
        "        \"indexes/doc_meta\",\n",
        "        \"indexes/docMeta\",\n",
        "        \"indexes/metadata\",\n",
        "        \"indexes/docs_meta\",\n",
        "    ]\n",
        "\n",
        "    txt = \"\"\n",
        "\n",
        "    for base in base_candidates:\n",
        "        try:\n",
        "            v = firebase_get(f\"{base}/{key}\")\n",
        "        except Exception:\n",
        "            v = None\n",
        "        v = _safe_str(v).strip()\n",
        "        if v:\n",
        "            txt = v\n",
        "            break\n",
        "\n",
        "    if not txt:\n",
        "        for base in meta_candidates:\n",
        "            for field in [\"abstract\", \"summary\", \"snippet\", \"title\"]:\n",
        "                try:\n",
        "                    v = firebase_get(f\"{base}/{key}/{field}\")\n",
        "                except Exception:\n",
        "                    v = None\n",
        "                v = _safe_str(v).strip()\n",
        "                if v:\n",
        "                    txt = v\n",
        "                    break\n",
        "            if txt:\n",
        "                break\n",
        "\n",
        "    _DOC_TEXT_CACHE[doc_id] = txt\n",
        "    return txt\n",
        "\n",
        "def _doc_term_freq(doc_id: int) -> Counter:\n",
        "    if doc_id in _DOC_TF_CACHE:\n",
        "        return _DOC_TF_CACHE[doc_id]\n",
        "\n",
        "    text = get_doc_text(doc_id)\n",
        "    terms = preprocess_query(text) if text else []\n",
        "    tf = Counter(terms)\n",
        "\n",
        "    _DOC_TF_CACHE[doc_id] = tf\n",
        "    _DOC_LEN_CACHE[doc_id] = int(sum(tf.values()))\n",
        "    return tf\n",
        "\n",
        "def _build_bm25_stats(doc_ids: List[int]) -> Dict[str, Any]:\n",
        "    cache_key = \"bm25_stats_v1\"\n",
        "    if cache_key in _BM25_STATS_CACHE:\n",
        "        return _BM25_STATS_CACHE[cache_key]\n",
        "\n",
        "    N = max(len(doc_ids), 1)\n",
        "    df = defaultdict(int)\n",
        "    doc_lens = []\n",
        "\n",
        "    for did in doc_ids:\n",
        "        tf = _doc_term_freq(did)\n",
        "        doc_lens.append(_DOC_LEN_CACHE.get(did, 0))\n",
        "        for term in tf.keys():\n",
        "            df[term] += 1\n",
        "\n",
        "    avgdl = (sum(doc_lens) / max(len(doc_lens), 1)) if doc_lens else 1.0\n",
        "    stats = {\"N\": N, \"df\": df, \"avgdl\": avgdl}\n",
        "\n",
        "    _BM25_STATS_CACHE[cache_key] = stats\n",
        "    return stats\n",
        "\n",
        "def bm25_rank(\n",
        "    query: str,\n",
        "    k: int = 3,\n",
        "    k1: float = 1.5,\n",
        "    b: float = 0.75\n",
        ") -> Tuple[List[str], List[Tuple[int, float]]]:\n",
        "    _ensure_stores_loaded()\n",
        "\n",
        "    q_terms = preprocess_query(query)\n",
        "    doc_ids = _all_doc_ids()\n",
        "    stats = _build_bm25_stats(doc_ids)\n",
        "\n",
        "    N = stats[\"N\"]\n",
        "    df = stats[\"df\"]\n",
        "    avgdl = stats[\"avgdl\"] if stats[\"avgdl\"] > 0 else 1.0\n",
        "\n",
        "    scores = defaultdict(float)\n",
        "\n",
        "    for did in doc_ids:\n",
        "        tf = _doc_term_freq(did)\n",
        "        dl = max(_DOC_LEN_CACHE.get(did, 0), 0)\n",
        "\n",
        "        s = 0.0\n",
        "        for term in q_terms:\n",
        "            f = tf.get(term, 0)\n",
        "            if f <= 0:\n",
        "                continue\n",
        "\n",
        "            term_df = df.get(term, 0)\n",
        "            idf = math.log((N - term_df + 0.5) / (term_df + 0.5) + 1.0)\n",
        "\n",
        "            denom = f + k1 * (1.0 - b + b * (dl / avgdl))\n",
        "            s += idf * (f * (k1 + 1.0)) / (denom if denom != 0 else 1.0)\n",
        "\n",
        "        if s != 0.0:\n",
        "            scores[did] = s\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[: max(k, 0)]\n",
        "    return q_terms, ranked\n",
        "\n",
        "# -------------------------\n",
        "# Gemini (google-genai) bridge\n",
        "# -------------------------\n",
        "\n",
        "_GEMINI_CLIENT = None\n",
        "_GEMINI_MODEL_ID = os.environ.get(\"GEMINI_MODEL_ID\", \"gemini-2.5-flash\")\n",
        "\n",
        "def _get_colab_secret(name: str) -> Optional[str]:\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(name)\n",
        "        if v:\n",
        "            return str(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _get_api_key() -> Optional[str]:\n",
        "    # Try Colab Secrets first (case-sensitive), then environment variables.\n",
        "    candidates = [\n",
        "        \"GEMINI_API_KEY\",\n",
        "        \"GOOGLE_API_KEY\",\n",
        "        \"GENAI_API_KEY\",\n",
        "        \"API_KEY\",\n",
        "    ]\n",
        "\n",
        "    for n in candidates:\n",
        "        v = _get_colab_secret(n)\n",
        "        if v:\n",
        "            return v.strip()\n",
        "\n",
        "    for n in candidates:\n",
        "        v = os.environ.get(n)\n",
        "        if v:\n",
        "            return str(v).strip()\n",
        "\n",
        "    return None\n",
        "\n",
        "def _init_gemini() -> None:\n",
        "    global _GEMINI_CLIENT\n",
        "    if _GEMINI_CLIENT is not None:\n",
        "        return\n",
        "\n",
        "    api_key = _get_api_key()\n",
        "    if not api_key:\n",
        "        raise NameError(\n",
        "            \"Gemini API key not found. Put it in Colab Secrets as GEMINI_API_KEY (recommended) \"\n",
        "            \"or set environment variable GEMINI_API_KEY.\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        from google import genai  # google-genai\n",
        "    except Exception as e:\n",
        "        raise ImportError(\n",
        "            \"google-genai is not installed. Run: pip install -q google-genai\"\n",
        "        ) from e\n",
        "\n",
        "    # Client can be created with api_key (recommended in SDK examples).\n",
        "    # Do NOT print the key.\n",
        "    _GEMINI_CLIENT = genai.Client(api_key=api_key)\n",
        "\n",
        "\n",
        "_LLM_CACHE: Dict[str, str] = {}\n",
        "\n",
        "def llm_generate(\n",
        "    prompt: str,\n",
        "    max_retries: int = 6,\n",
        "    base_sleep: float = 0.8,\n",
        "    max_sleep: float = 8.0,\n",
        "    use_cache: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Gemini call with retry/backoff for 503/429 + simple cache.\n",
        "    \"\"\"\n",
        "    _init_gemini()\n",
        "\n",
        "    p = prompt if isinstance(prompt, str) else str(prompt)\n",
        "\n",
        "    if use_cache and p in _LLM_CACHE:\n",
        "        return _LLM_CACHE[p]\n",
        "\n",
        "    last_err = None\n",
        "\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            resp = _GEMINI_CLIENT.models.generate_content(\n",
        "                model=_GEMINI_MODEL_ID,\n",
        "                contents=p\n",
        "            )\n",
        "            out = _safe_str(getattr(resp, \"text\", \"\")).strip()\n",
        "            if use_cache:\n",
        "                _LLM_CACHE[p] = out\n",
        "            return out\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            msg = str(e)\n",
        "\n",
        "            # Retry on common transient overload/limit errors\n",
        "            retryable = (\"503\" in msg) or (\"429\" in msg) or (\"UNAVAILABLE\" in msg) or (\"overloaded\" in msg.lower())\n",
        "\n",
        "            if (not retryable) or (attempt >= max_retries):\n",
        "                raise\n",
        "\n",
        "            sleep_s = min(max_sleep, base_sleep * (2 ** attempt))\n",
        "            sleep_s *= (0.7 + 0.6 * random.random())  # jitter\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "    raise last_err\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDmVbDIzdSFr"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 11. RAG Core (Retrieval + Chunking + Evidence extraction with Gemini)\n",
        "# =========================\n",
        "def search_top_k(query: str, k: int = 3) -> Tuple[List[str], List[Tuple[int, float]]]:\n",
        "    return bm25_rank(query, k=k)\n",
        "\n",
        "def _make_sources(doc_ids: List[int]) -> List[str]:\n",
        "    srcs = []\n",
        "    for did in doc_ids:\n",
        "        u = _get_source_url(did)\n",
        "        if u:\n",
        "            srcs.append(u)\n",
        "    return srcs\n",
        "\n",
        "def _chunk_text(text: str, max_chars: int = 6000) -> List[str]:\n",
        "    t = _safe_str(text)\n",
        "    if not t:\n",
        "        return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    n = len(t)\n",
        "    while i < n:\n",
        "        j = min(i + max_chars, n)\n",
        "        chunks.append(t[i:j])\n",
        "        i = j\n",
        "    return chunks\n",
        "\n",
        "def _doc_title(doc_id: int) -> str:\n",
        "    txt = get_doc_text(doc_id)\n",
        "    m = re.search(r\"^TITLE:\\s*(.+)$\", txt, flags=re.MULTILINE)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    return f\"DOC {doc_id}\"\n",
        "\n",
        "def rag_retrieve(query: str, k: int = 3) -> Dict[str, Any]:\n",
        "    q_terms, ranked = search_top_k(query, k=k)\n",
        "    doc_ids = [doc_id for (doc_id, _score) in ranked]\n",
        "    sources = _make_sources(doc_ids)\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"q_terms\": q_terms,\n",
        "        \"ranked\": ranked,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"sources\": sources,\n",
        "        \"top_doc_id\": (doc_ids[0] if doc_ids else None),\n",
        "    }\n",
        "\n",
        "def _select_chunks_by_overlap(query_terms: List[str], chunks: List[str], max_keep: int = 4) -> List[Tuple[int, str, int]]:\n",
        "    q = set(query_terms or [])\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    scored: List[Tuple[int, str, int]] = []\n",
        "    for i, ch in enumerate(chunks):\n",
        "        terms = set(preprocess_query(ch) or [])\n",
        "        ov = len(q.intersection(terms))\n",
        "        scored.append((i, ch, ov))\n",
        "\n",
        "    scored.sort(key=lambda x: (-x[2], x[0]))\n",
        "\n",
        "    # If overlap is weak (common in \"objective\", \"motivation\" questions),\n",
        "    # sample diverse chunks to avoid missing the right section.\n",
        "    if scored and scored[0][2] == 0:\n",
        "        n = len(chunks)\n",
        "        idxs = []\n",
        "        for cand in [0, 1, max(0, n // 3), max(0, (2 * n) // 3), n - 1]:\n",
        "            if 0 <= cand < n and cand not in idxs:\n",
        "                idxs.append(cand)\n",
        "        idxs = idxs[:max_keep]\n",
        "        return [(i, chunks[i], 0) for i in idxs]\n",
        "\n",
        "    return scored[:max_keep]\n",
        "\n",
        "def _extract_evidence_from_chunk(question: str, doc_id: int, chunk_id: int, chunk_text: str) -> Dict[str, Any]:\n",
        "    prompt = (\n",
        "        \"You are an information extraction system.\\n\"\n",
        "        \"Task: Decide if the CHUNK contains information that answers the QUESTION.\\n\\n\"\n",
        "        \"Return JSON only with fields:\\n\"\n",
        "        \"- found: boolean\\n\"\n",
        "        \"- answer: string (empty if found=false)\\n\"\n",
        "        \"- evidence: array of up to 3 verbatim sentences copied from the CHUNK\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Use ONLY sentences that appear verbatim in the CHUNK.\\n\"\n",
        "        \"- If the answer is not stated in the CHUNK, set found=false.\\n\"\n",
        "        \"- Do NOT mention any other papers, models, or methods not in the CHUNK.\\n\\n\"\n",
        "        f\"QUESTION:\\n{question}\\n\\n\"\n",
        "        f\"CHUNK (DOC {doc_id} | CHUNK {chunk_id}):\\n{chunk_text}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        raw = llm_generate(prompt)\n",
        "    except Exception:\n",
        "        # If Gemini is overloaded even after retries, treat as \"no evidence\" for this chunk.\n",
        "        return {\"found\": False, \"answer\": \"\", \"evidence\": [], \"doc_id\": doc_id, \"chunk_id\": chunk_id}\n",
        "\n",
        "    obj = None\n",
        "    try:\n",
        "        obj = json.loads(raw)\n",
        "    except Exception:\n",
        "        m = re.search(r\"\\{.*\\}\", raw, flags=re.DOTALL)\n",
        "        if m:\n",
        "            try:\n",
        "                obj = json.loads(m.group(0))\n",
        "            except Exception:\n",
        "                obj = None\n",
        "\n",
        "    if not isinstance(obj, dict):\n",
        "        return {\"found\": False, \"answer\": \"\", \"evidence\": [], \"doc_id\": doc_id, \"chunk_id\": chunk_id}\n",
        "\n",
        "    found = bool(obj.get(\"found\", False))\n",
        "    answer = _safe_str(obj.get(\"answer\", \"\")).strip()\n",
        "    evidence = obj.get(\"evidence\", [])\n",
        "    if not isinstance(evidence, list):\n",
        "        evidence = []\n",
        "\n",
        "    evidence = [_safe_str(x).strip() for x in evidence if _safe_str(x).strip()]\n",
        "    evidence = evidence[:3]\n",
        "\n",
        "    # Require evidence to avoid hallucinations\n",
        "    if not found or not evidence:\n",
        "        return {\"found\": False, \"answer\": \"\", \"evidence\": [], \"doc_id\": doc_id, \"chunk_id\": chunk_id}\n",
        "\n",
        "    return {\"found\": True, \"answer\": answer, \"evidence\": evidence, \"doc_id\": doc_id, \"chunk_id\": chunk_id}\n",
        "\n",
        "def _final_answer_from_evidence(question: str, evidence_packets: List[Dict[str, Any]]) -> str:\n",
        "    # Always return English only; use a strict sentinel when not supported.\n",
        "    NO = \"NO_ANSWER_FOUND\"\n",
        "\n",
        "    if not evidence_packets:\n",
        "        return NO\n",
        "\n",
        "    lines = []\n",
        "    for p in evidence_packets:\n",
        "        did = p[\"doc_id\"]\n",
        "        cid = p[\"chunk_id\"]\n",
        "        for s in p.get(\"evidence\", []):\n",
        "            lines.append(f\"[DOC {did} | CHUNK {cid}] {s}\")\n",
        "    evidence_text = \"\\n\".join(lines).strip()\n",
        "\n",
        "    prompt = (\n",
        "        \"Answer in ENGLISH ONLY.\\n\"\n",
        "        \"Do NOT use any Hebrew characters.\\n\"\n",
        "        \"Use ONLY the evidence sentences provided.\\n\"\n",
        "        f\"If the evidence does not answer the question, output exactly: {NO}\\n\"\n",
        "        \"Do NOT add any extra models/methods/claims not in the evidence.\\n\\n\"\n",
        "        f\"Question:\\n{question}\\n\\n\"\n",
        "        f\"Evidence:\\n{evidence_text}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        out = llm_generate(prompt).strip()\n",
        "    except Exception:\n",
        "        return NO\n",
        "\n",
        "    # If Gemini outputs Hebrew or empty or refuses -> retry once with stronger instruction\n",
        "    if (not out) or (out == NO) or re.search(r\"[\\u0590-\\u05FF]\", out):\n",
        "        retry = (\n",
        "            \"You MUST output ENGLISH ONLY (ASCII/English letters).\\n\"\n",
        "            f\"If you cannot answer from evidence, output exactly: {NO}\\n\\n\"\n",
        "            f\"Question:\\n{question}\\n\\n\"\n",
        "            f\"Evidence:\\n{evidence_text}\\n\\n\"\n",
        "            \"Answer:\"\n",
        "        )\n",
        "        try:\n",
        "            out2 = llm_generate(retry).strip()\n",
        "            if out2 and (out2 != NO) and (not re.search(r\"[\\u0590-\\u05FF]\", out2)):\n",
        "                out = out2\n",
        "            else:\n",
        "                out = NO\n",
        "        except Exception:\n",
        "            out = NO\n",
        "\n",
        "    # Final guard\n",
        "    if (not out) or (out == NO) or re.search(r\"[\\u0590-\\u05FF]\", out):\n",
        "        return NO\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _support_score(evidence_packets: List[Dict[str, Any]]) -> int:\n",
        "    score = 0\n",
        "    for p in evidence_packets:\n",
        "        score += len(p.get(\"evidence\", []) or [])\n",
        "    return int(score)\n",
        "\n",
        "def rag_answer_with_model(\n",
        "    query: str,\n",
        "    k: int = 3,\n",
        "    chunk_chars: int = 6000,\n",
        "    max_chunks_per_doc: int = 4,\n",
        "    max_docs_to_try: int = 3,\n",
        "    min_support_to_accept: int = 2,\n",
        "    max_total_llm_calls: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    res = rag_retrieve(query, k=k)\n",
        "    ranked = (res.get(\"ranked\", []) or [])[:max_docs_to_try]\n",
        "\n",
        "    per_doc = []\n",
        "    chosen = None\n",
        "    llm_calls = 0\n",
        "\n",
        "    # Try docs sequentially; stop early when evidence is strong enough\n",
        "    for did, sc in ranked:\n",
        "        full_text = get_doc_text(int(did))\n",
        "        chunks = _chunk_text(full_text, max_chars=chunk_chars)\n",
        "        picked = _select_chunks_by_overlap(res.get(\"q_terms\", []), chunks, max_keep=max_chunks_per_doc)\n",
        "\n",
        "        ev_packets: List[Dict[str, Any]] = []\n",
        "        for idx, ch, _ov in picked:\n",
        "            if llm_calls >= max_total_llm_calls:\n",
        "                break\n",
        "            llm_calls += 1\n",
        "            ev = _extract_evidence_from_chunk(query, int(did), int(idx + 1), ch)\n",
        "            if ev.get(\"found\"):\n",
        "                ev_packets.append(ev)\n",
        "                if _support_score(ev_packets) >= min_support_to_accept:\n",
        "                    # Good enough -> stop scanning more chunks\n",
        "                    break\n",
        "\n",
        "        item = {\n",
        "            \"doc_id\": int(did),\n",
        "            \"bm25\": float(sc),\n",
        "            \"title\": _doc_title(int(did)),\n",
        "            \"evidence\": ev_packets,\n",
        "            \"support\": _support_score(ev_packets),\n",
        "        }\n",
        "        per_doc.append(item)\n",
        "\n",
        "        if item[\"support\"] >= min_support_to_accept:\n",
        "            chosen = item\n",
        "            break\n",
        "\n",
        "        if llm_calls >= max_total_llm_calls:\n",
        "            break\n",
        "\n",
        "    # If nothing reached threshold, pick best evidence doc (if any evidence exists)\n",
        "    if chosen is None and per_doc:\n",
        "        per_doc.sort(key=lambda x: (-x[\"support\"], -x[\"bm25\"], x[\"doc_id\"]))\n",
        "        if per_doc[0][\"support\"] > 0:\n",
        "            chosen = per_doc[0]\n",
        "\n",
        "    if chosen is None or chosen[\"support\"] <= 0:\n",
        "        return {\n",
        "            \"answer\": \"NO_ANSWER_FOUND\",\n",
        "            \"sources\": _make_sources([int(d) for (d, _s) in ranked]),\n",
        "            \"ranked\": ranked,\n",
        "            \"doc_ids\": [int(d) for (d, _s) in ranked],\n",
        "            \"evidence\": [],\n",
        "            \"top_doc_id\": (int(ranked[0][0]) if ranked else None),\n",
        "            \"picked_doc\": None,\n",
        "            \"per_doc_support\": per_doc,\n",
        "        }\n",
        "\n",
        "    picked_doc_id = chosen[\"doc_id\"]\n",
        "    answer = _final_answer_from_evidence(query, chosen[\"evidence\"])\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"sources\": _make_sources([picked_doc_id]),\n",
        "        \"ranked\": ranked,\n",
        "        \"doc_ids\": [int(d) for (d, _s) in ranked],\n",
        "        \"evidence\": chosen[\"evidence\"],\n",
        "        \"top_doc_id\": picked_doc_id,\n",
        "        \"picked_doc\": {\n",
        "            \"doc_id\": picked_doc_id,\n",
        "            \"title\": chosen[\"title\"],\n",
        "            \"bm25\": chosen[\"bm25\"],\n",
        "            \"support\": chosen[\"support\"],\n",
        "        },\n",
        "        \"per_doc_support\": per_doc,\n",
        "        \"llm_calls\": llm_calls,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXsIyqVwdT-C"
      },
      "source": [
        "TAB 5 GUI - RAG Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjpGYYp8dUSJ"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 12. UI Adapter + Tab Builder (Gradio)\n",
        "# =========================\n",
        "\n",
        "def rag_ui(query: str, k: int = 3) -> tuple:\n",
        "    out = rag_answer_with_model(\n",
        "        query,\n",
        "        k=k,\n",
        "        chunk_chars=6000,\n",
        "        max_chunks_per_doc=4,\n",
        "        max_docs_to_try=3,\n",
        "        min_support_to_accept=2,\n",
        "        max_total_llm_calls=10,\n",
        "    )\n",
        "\n",
        "    answer = _safe_str(out.get(\"answer\", \"\")).strip() or \"◊ú◊ê ◊†◊û◊¶◊ê◊î ◊™◊©◊ï◊ë◊î.\"\n",
        "    ranked = out.get(\"ranked\", []) or []\n",
        "    sources = out.get(\"sources\", []) or []\n",
        "    evidence = out.get(\"evidence\", []) or []\n",
        "    picked = out.get(\"picked_doc\", None)\n",
        "    per_doc = out.get(\"per_doc_support\", []) or []\n",
        "    llm_calls = out.get(\"llm_calls\", None)\n",
        "\n",
        "    src_lines = []\n",
        "    if picked:\n",
        "        src_lines.append(f\"Picked doc: {picked['doc_id']} | support={picked['support']} | bm25={picked['bm25']:.4f}\")\n",
        "        src_lines.append(f\"Title: {picked['title']}\")\n",
        "\n",
        "    if llm_calls is not None:\n",
        "        src_lines.append(f\"LLM calls used: {llm_calls}\")\n",
        "\n",
        "    if ranked:\n",
        "        src_lines.append(\"\")\n",
        "        src_lines.append(\"Ranking:\")\n",
        "        for did, sc in ranked:\n",
        "            src_lines.append(f\"- DOC {did}: {sc:.4f} | {_doc_title(int(did))}\")\n",
        "\n",
        "    if per_doc:\n",
        "        src_lines.append(\"\")\n",
        "        src_lines.append(\"Support per doc (evidence sentences):\")\n",
        "        for d in per_doc:\n",
        "            src_lines.append(f\"- DOC {d['doc_id']}: support={d['support']} | bm25={d['bm25']:.4f}\")\n",
        "\n",
        "    if sources:\n",
        "        src_lines.append(\"\")\n",
        "        src_lines.append(\"Sources:\")\n",
        "        for u in sources:\n",
        "            src_lines.append(f\"- {u}\")\n",
        "\n",
        "    if evidence:\n",
        "        src_lines.append(\"\")\n",
        "        src_lines.append(\"Evidence packets:\")\n",
        "        for p in evidence[:10]:\n",
        "            did = p.get(\"doc_id\")\n",
        "            cid = p.get(\"chunk_id\")\n",
        "            src_lines.append(f\"- DOC {did} | CHUNK {cid}: {len(p.get('evidence', []))} sentences\")\n",
        "\n",
        "    return answer, \"\\n\".join(src_lines).strip()\n",
        "\n",
        "def build_rag_chat_tab():\n",
        "    import gradio as gr\n",
        "\n",
        "    gr.Markdown(\"### RAG Chat (Gemini)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        q = gr.Textbox(label=\"Question\", placeholder=\"Ask a question about the indexed papers...\", lines=2)\n",
        "    with gr.Row():\n",
        "        k = gr.Slider(1, 5, value=3, step=1, label=\"Top-K documents\")\n",
        "\n",
        "    ask = gr.Button(\"Search\")\n",
        "    out_answer = gr.Textbox(label=\"Answer (Gemini)\", lines=8)\n",
        "    out_sources = gr.Textbox(label=\"Sources\", lines=14)\n",
        "\n",
        "    ask.click(fn=rag_ui, inputs=[q, k], outputs=[out_answer, out_sources])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TAB 6 LOGIC - Chat conversation\n",
        "\n"
      ],
      "metadata": {
        "id": "8BowsTl5AzP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "def build_system_instruction() -> str:\n",
        "    return \"You are a helpful, friendly chatbot. Answer clearly. Keep a natural conversation.\"\n",
        "\n",
        "def _get_api_key() -> str:\n",
        "    # English comments only\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        k = userdata.get(\"GEMINI_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
        "    except Exception:\n",
        "        k = None\n",
        "\n",
        "    k = k or os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if not k:\n",
        "        raise RuntimeError(\"Missing GEMINI_API_KEY (Colab Secrets) or env var.\")\n",
        "    return k\n",
        "\n",
        "client = genai.Client(api_key=_get_api_key())"
      ],
      "metadata": {
        "id": "23xQniSxA1xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_turn(user_message: str, history: list[tuple[str, str]], temperature: float = 0.7):\n",
        "    # English comments only\n",
        "    user_message = (user_message or \"\").strip()\n",
        "    if not user_message:\n",
        "        return \"\", history, history\n",
        "\n",
        "    contents = []\n",
        "\n",
        "    # Add past turns\n",
        "    for u, a in history:\n",
        "        contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=u)]))\n",
        "        contents.append(types.Content(role=\"model\", parts=[types.Part.from_text(text=a)]))\n",
        "\n",
        "    # Add new user message\n",
        "    contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=user_message)]))\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(\n",
        "            system_instruction=build_system_instruction(),\n",
        "            temperature=temperature,\n",
        "            max_output_tokens=512,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    answer = (resp.text or \"\").strip()\n",
        "    if not answer:\n",
        "        answer = \"I couldn't generate an answer. Try again.\"\n",
        "\n",
        "    new_history = history + [(user_message, answer)]\n",
        "    return \"\", new_history, new_history\n"
      ],
      "metadata": {
        "id": "zR_mk5o5A8G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TAB 6 GUI - Chat conversation\n",
        "\n"
      ],
      "metadata": {
        "id": "QaWdGlHLBBPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gemini_chat_tab():\n",
        "    gr.Markdown(\"## üí¨ Gemini Free Chat\")\n",
        "    gr.Markdown(\"Free conversation powered by Gemini (multi-turn, per-session history).\")\n",
        "\n",
        "    chat = gr.Chatbot(label=\"Chat\")\n",
        "    state = gr.State([])  # list[tuple[user, assistant]]\n",
        "\n",
        "    msg = gr.Textbox(label=\"Message\", placeholder=\"Type here...\", lines=2)\n",
        "    temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.05, label=\"Creativity\")\n",
        "\n",
        "    with gr.Row():\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=gemini_turn,\n",
        "        inputs=[msg, state, temperature],\n",
        "        outputs=[msg, chat, state],\n",
        "    )\n",
        "\n",
        "    msg.submit(\n",
        "        fn=gemini_turn,\n",
        "        inputs=[msg, state, temperature],\n",
        "        outputs=[msg, chat, state],\n",
        "    )\n",
        "\n",
        "    def clear_chat():\n",
        "        # English comments only\n",
        "        return [], []\n",
        "\n",
        "    clear_btn.click(fn=clear_chat, inputs=[], outputs=[chat, state])\n"
      ],
      "metadata": {
        "id": "C1lTVGMQBCsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TAB 7 LOGIC - üéÆ Farm Rewards (Gamification)"
      ],
      "metadata": {
        "id": "nSheZbYYkpE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# LOGIC ‚Äî Mission Rewards System (Gamification)\n",
        "# =========================================================\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1) Configuration + Firebase Storage Path\n",
        "#    - Where we store the profile (global, no username)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "TZ_NAME = \"Asia/Jerusalem\"\n",
        "GAMIFICATION_REF = db.reference(\"gamification/global\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) Time Helpers\n",
        "#    - Used for \"once per day\" missions and timestamps\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def _today_key():\n",
        "    from datetime import datetime\n",
        "    from zoneinfo import ZoneInfo\n",
        "    return datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def _now_iso():\n",
        "    from datetime import datetime\n",
        "    from zoneinfo import ZoneInfo\n",
        "    return datetime.now(ZoneInfo(TZ_NAME)).isoformat()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3) Default Profile Model\n",
        "#    - The schema we expect to exist in Firebase\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "DEFAULT_PROFILE = {\n",
        "    \"points\": 0,\n",
        "    \"spins_available\": 0,  # 1 spin token per completed mission\n",
        "    \"missions\": {\n",
        "        \"sync_data\": {\"last_completed\": None, \"total_completed\": 0},\n",
        "        \"analyze_plant\": {\"last_completed\": None, \"total_completed\": 0},\n",
        "        \"generate_report\": {\"last_completed\": None, \"total_completed\": 0},\n",
        "    },\n",
        "    \"coupons\": []  # list of dicts: {code,label,created_at,redeemed}\n",
        "}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4) Profile I/O (Read / Write)\n",
        "#    - Loads from Firebase and merges missing keys safely\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def _get_profile():\n",
        "    p = GAMIFICATION_REF.get() or {}\n",
        "\n",
        "    prof = {\n",
        "        \"points\": int(p.get(\"points\", DEFAULT_PROFILE[\"points\"])),\n",
        "        \"spins_available\": int(p.get(\"spins_available\", DEFAULT_PROFILE[\"spins_available\"])),\n",
        "        \"missions\": p.get(\"missions\", {}) or {},\n",
        "        \"coupons\": p.get(\"coupons\", []) or [],\n",
        "    }\n",
        "\n",
        "    # Merge default missions safely (ensures all mission keys exist)\n",
        "    merged = {}\n",
        "    for mid, base in DEFAULT_PROFILE[\"missions\"].items():\n",
        "        m = (prof[\"missions\"].get(mid) or {})\n",
        "        merged[mid] = {\n",
        "            \"last_completed\": m.get(\"last_completed\", base[\"last_completed\"]),\n",
        "            \"total_completed\": int(m.get(\"total_completed\", base[\"total_completed\"])),\n",
        "        }\n",
        "    prof[\"missions\"] = merged\n",
        "    return prof\n",
        "\n",
        "def _save_profile(prof: dict):\n",
        "    GAMIFICATION_REF.set(prof)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5) Mission Completion Rule (Once per Day)\n",
        "#    - Adds points + 1 spin if not completed today\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def complete_mission(mission_id: str, points: int):\n",
        "    \"\"\"\n",
        "    Returns: (prof, earned_today: bool)\n",
        "    Logic: once-per-day per mission to prevent spam clicking.\n",
        "    Reward: +points AND +1 spin token.\n",
        "    \"\"\"\n",
        "    prof = _get_profile()\n",
        "    today = _today_key()\n",
        "\n",
        "    m = prof[\"missions\"].get(mission_id, {\"last_completed\": None, \"total_completed\": 0})\n",
        "    if m.get(\"last_completed\") == today:\n",
        "        return prof, False\n",
        "\n",
        "    prof[\"points\"] = int(prof.get(\"points\", 0)) + int(points)\n",
        "    prof[\"spins_available\"] = int(prof.get(\"spins_available\", 0)) + 1\n",
        "\n",
        "    m[\"last_completed\"] = today\n",
        "    m[\"total_completed\"] = int(m.get(\"total_completed\", 0)) + 1\n",
        "    prof[\"missions\"][mission_id] = m\n",
        "\n",
        "    _save_profile(prof)\n",
        "    return prof, True\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6) Wheel Rewards (Spin Logic)\n",
        "#    - Consumes 1 spin token and grants points/coupon\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "WHEEL_REWARDS = [\n",
        "    (\"+5 points\",  {\"points\": 5}),\n",
        "    (\"+10 points\", {\"points\": 10}),\n",
        "    (\"+20 points\", {\"points\": 20}),\n",
        "    (\"Coupon: 5% off\",  {\"coupon\": {\"base\": \"CG-5OFF\", \"label\": \"5% off\"}}),\n",
        "    (\"Coupon: 10% off\", {\"coupon\": {\"base\": \"CG-10OFF\",\"label\": \"10% off\"}}),\n",
        "]\n",
        "\n",
        "def spin_wheel():\n",
        "    \"\"\"\n",
        "    Consumes 1 spin token and gives either points or a coupon.\n",
        "    Returns: (message, prof)\n",
        "    \"\"\"\n",
        "    prof = _get_profile()\n",
        "    spins = int(prof.get(\"spins_available\", 0))\n",
        "    if spins <= 0:\n",
        "        return \"No spins available. Complete a mission to earn a spin!\", prof\n",
        "\n",
        "    prof[\"spins_available\"] = spins - 1\n",
        "\n",
        "    label, payload = random.choice(WHEEL_REWARDS)\n",
        "\n",
        "    if \"points\" in payload:\n",
        "        prof[\"points\"] = int(prof.get(\"points\", 0)) + int(payload[\"points\"])\n",
        "\n",
        "    if \"coupon\" in payload:\n",
        "        # Make coupon code unique (so you can redeem multiple coupons)\n",
        "        from datetime import datetime\n",
        "        from zoneinfo import ZoneInfo\n",
        "        suffix = datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%H%M%S\")\n",
        "        code = f\"{payload['coupon']['base']}-{suffix}\"\n",
        "\n",
        "        prof[\"coupons\"].append({\n",
        "            \"code\": code,\n",
        "            \"label\": payload[\"coupon\"][\"label\"],\n",
        "            \"created_at\": _now_iso(),\n",
        "            \"redeemed\": False,\n",
        "        })\n",
        "\n",
        "    _save_profile(prof)\n",
        "    return f\"üé° You got: {label}\", prof\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7) Voucher Redemption by Points (Redeem Logic)\n",
        "#    - User chooses a tier, points are deducted, voucher created\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "REDEEM_TIERS = {\n",
        "    \"5‚Ç™ Voucher (100 pts)\": 100,\n",
        "    \"10‚Ç™ Voucher (200 pts)\": 200,\n",
        "    \"20‚Ç™ Voucher (350 pts)\": 350,\n",
        "}\n",
        "\n",
        "def redeem_voucher(tier_label: str):\n",
        "    \"\"\"\n",
        "    On Redeem click:\n",
        "    - If enough points: subtract cost, create voucher (coupon) and save to Firebase.\n",
        "    - Else: return message only.\n",
        "    Returns: (message, prof)\n",
        "    \"\"\"\n",
        "    prof = _get_profile()\n",
        "\n",
        "    if tier_label not in REDEEM_TIERS:\n",
        "        return \"Please select a voucher tier.\", prof\n",
        "\n",
        "    cost = int(REDEEM_TIERS[tier_label])\n",
        "    if int(prof.get(\"points\", 0)) < cost:\n",
        "        missing = cost - int(prof.get(\"points\", 0))\n",
        "        return f\"Not enough points. You need {missing} more.\", prof\n",
        "\n",
        "    # Create a simple unique voucher code\n",
        "    suffix = datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%H%M%S\")\n",
        "    code = f\"VOUCH-{suffix}\"\n",
        "\n",
        "    prof[\"points\"] = int(prof.get(\"points\", 0)) - cost\n",
        "    prof[\"coupons\"].append({\n",
        "        \"code\": code,\n",
        "        \"label\": tier_label.split(\" (\")[0],  # \"5‚Ç™ Voucher\"\n",
        "        \"created_at\": _now_iso(),\n",
        "        \"redeemed\": True,  # since user redeemed it now\n",
        "    })\n",
        "\n",
        "    _save_profile(prof)\n",
        "    return f\"‚úÖ Voucher created! Code: {code}\", prof\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8) Formatting Helpers (for UI display)\n",
        "#    - Converts profile data into nice text/markdown\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def _format_missions_md(prof: dict) -> str:\n",
        "    today = _today_key()\n",
        "    m = prof.get(\"missions\", {}) or {}\n",
        "\n",
        "    def row(mid, title):\n",
        "        last = (m.get(mid, {}) or {}).get(\"last_completed\")\n",
        "        total = (m.get(mid, {}) or {}).get(\"total_completed\", 0)\n",
        "        done = (last == today)\n",
        "        return f\"- **{title}**: {'‚úÖ Done today' if done else '‚¨ú Not done today'} (total: {total})\"\n",
        "\n",
        "    lines = [\n",
        "        \"### Daily Missions\",\n",
        "        row(\"sync_data\", \"Sync New Data\"),\n",
        "        row(\"analyze_plant\", \"Analyze a Plant Image\"),\n",
        "        row(\"generate_report\", \"Generate a Report\"),\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def _format_coupons_text(prof: dict) -> str:\n",
        "    coupons = prof.get(\"coupons\", []) or []\n",
        "    if not coupons:\n",
        "        return \"No coupons yet.\"\n",
        "\n",
        "    lines = []\n",
        "    for c in coupons[-15:][::-1]:\n",
        "        status = \"REDEEMED\" if c.get(\"redeemed\") else \"ACTIVE\"\n",
        "        lines.append(f\"{c.get('code')} | {c.get('label','Coupon')} | {status}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 9) UI Handlers (Gradio outputs)\n",
        "#    - Functions that return values matching Gradio outputs\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def rewards_refresh():\n",
        "    prof = _get_profile()\n",
        "    return (\n",
        "        str(prof.get(\"points\", 0)),\n",
        "        str(prof.get(\"spins_available\", 0)),\n",
        "        _format_missions_md(prof),\n",
        "        _format_coupons_text(prof),\n",
        "    )\n",
        "\n",
        "def rewards_spin():\n",
        "    msg, _ = spin_wheel()\n",
        "    pts, spins, missions_md, coupons_txt = rewards_refresh()\n",
        "    return msg, pts, spins, missions_md, coupons_txt\n",
        "\n",
        "def rewards_redeem(tier_label: str):\n",
        "    msg, _ = redeem_voucher(tier_label)\n",
        "    pts, spins, missions_md, coupons_txt = rewards_refresh()\n",
        "    return msg, pts, spins, missions_md, coupons_txt\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 10) Mission Wrappers (connect app actions -> mission rewards)\n",
        "#     - Wrap existing app functions and award missions if valid\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def sync_screen_gamified():\n",
        "    msg, count = sync_new_data_from_server()\n",
        "    # Mission completed only if new rows were actually saved\n",
        "    if count and count > 0:\n",
        "        prof, earned = complete_mission(\"sync_data\", points=10)\n",
        "        if earned:\n",
        "            msg += f\"\\nüéÆ Rewards: +10 points, +1 spin (Spins now: {prof['spins_available']})\"\n",
        "    return msg\n",
        "\n",
        "def analyze_plant_gamified(image, temp, humidity, soil):\n",
        "    diagnosis, status_html, alerts, recommendations = analyze_plant(image, temp, humidity, soil)\n",
        "\n",
        "    # Mission completed only if an image was actually provided\n",
        "    if image:\n",
        "        prof, earned = complete_mission(\"analyze_plant\", points=8)\n",
        "        if earned:\n",
        "            recommendations = (recommendations or \"\")\n",
        "            recommendations += f\"\\n\\nüéÆ Rewards: +8 points, +1 spin (Spins now: {prof['spins_available']})\"\n",
        "\n",
        "    return diagnosis, status_html, alerts, recommendations\n",
        "\n",
        "def generate_report_screen_gamified(limit: int):\n",
        "    status, file_path = generate_report_screen(limit)\n",
        "    # Mission completed only if report file was actually generated\n",
        "    if file_path:\n",
        "        prof, earned = complete_mission(\"generate_report\", points=12)\n",
        "        if earned:\n",
        "            status += f\" | üéÆ +12 points, +1 spin (Spins now: {prof['spins_available']})\"\n",
        "    return status, file_path\n"
      ],
      "metadata": {
        "id": "cYezmE4LkuuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ APP BUILDER"
      ],
      "metadata": {
        "id": "9MAtxIWckkZ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEj9G-EnMWTN"
      },
      "outputs": [],
      "source": [
        "def build_placeholder_tab(title: str, note: str = \"◊õ◊ê◊ü ◊ô◊ô◊õ◊†◊° ◊î◊ß◊ï◊ì ◊ë◊î◊û◊©◊ö\"):\n",
        "    gr.Markdown(f\"## {title}\")\n",
        "    gr.Markdown(note)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# TABS (EMPTY PLACEHOLDERS)\n",
        "# -----------------------------\n",
        "\n",
        "def build_iot_dashboard_tab():\n",
        "    \"\"\"Complete IoT Dashboard with all visualizations\"\"\"\n",
        "\n",
        "    gr.Markdown('### üìà Comprehensive Sensor Analytics')\n",
        "\n",
        "    refresh_btn = gr.Button('üîÑ Refresh All Data', variant='primary', size='lg')\n",
        "\n",
        "    # KPI Cards\n",
        "    gr.Markdown('#### üìå Current Readings')\n",
        "    kpi_html = gr.HTML()\n",
        "\n",
        "    # Statistics Cards\n",
        "    gr.Markdown('#### üìä Statistical Summary')\n",
        "    stats_html = gr.HTML()\n",
        "\n",
        "    # Main Time Series\n",
        "    gr.Markdown('#### üìà Time Series Overview')\n",
        "    ts_plot = gr.Plot()\n",
        "\n",
        "    # Correlation Analysis\n",
        "    gr.Markdown('#### üîó Correlation Analysis')\n",
        "    corr_card = gr.HTML()\n",
        "    corr_plot = gr.Plot()\n",
        "\n",
        "    # Hourly Patterns\n",
        "    gr.Markdown('#### ‚è∞ Hourly Patterns')\n",
        "    hourly_card = gr.HTML()\n",
        "    hourly_plot = gr.Plot()\n",
        "\n",
        "    # Daily Trends\n",
        "    gr.Markdown('#### üìÖ Daily Trends')\n",
        "    daily_card = gr.HTML()\n",
        "    daily_plot = gr.Plot()\n",
        "\n",
        "    # Distribution Analysis\n",
        "    gr.Markdown('#### üìä Distribution Analysis (Histograms)')\n",
        "    dist_card = gr.HTML()\n",
        "    dist_plot = gr.Plot()\n",
        "\n",
        "    # Moving Averages\n",
        "    gr.Markdown('#### üìâ Moving Averages')\n",
        "    with gr.Row():\n",
        "        ma_variable = gr.Dropdown(\n",
        "            choices=['temperature', 'humidity', 'soil'],\n",
        "            value='temperature',\n",
        "            label='Select Variable'\n",
        "        )\n",
        "        ma_btn = gr.Button('Generate Moving Average')\n",
        "    ma_card = gr.HTML()\n",
        "    ma_plot = gr.Plot()\n",
        "\n",
        "    # Wire up refresh button (11 outputs - without scatter)\n",
        "    refresh_btn.click(\n",
        "        dashboard_screen,\n",
        "        outputs=[\n",
        "            kpi_html, stats_html, ts_plot,\n",
        "            corr_card, corr_plot,\n",
        "            hourly_card, hourly_plot,\n",
        "            daily_card, daily_plot,\n",
        "            dist_card, dist_plot\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Wire up moving average\n",
        "    ma_btn.click(\n",
        "        dashboard_moving_avg,\n",
        "        inputs=ma_variable,\n",
        "        outputs=[ma_card, ma_plot]\n",
        "    )\n",
        "\n",
        "\n",
        "def build_search_engine_tab():\n",
        "    build_placeholder_tab(\"üîç Search Engine\")\n",
        "\n",
        "#def build_rag_chat_tab():\n",
        " #   build_placeholder_tab(\"üí¨ RAG Chat\")\n",
        "\n",
        "def build_sync_data_tab():\n",
        "    \"\"\"Sync data from server to Firebase\"\"\"\n",
        "\n",
        "    gr.Markdown('Sync Data from to Server')\n",
        "    gr.Markdown('Upload IoT Data to FireBase')\n",
        "\n",
        "    sync_btn = gr.Button('üîÑ Sync New Data', variant='primary', size='lg')\n",
        "    sync_output = gr.Textbox(label='Status', lines=5)\n",
        "\n",
        "    sync_btn.click(sync_screen_gamified, outputs=sync_output)\n",
        "\n",
        "def build_rewards_tab():\n",
        "    # ---------------------------------------------------------\n",
        "    # 1) Load current profile snapshot (initial values)\n",
        "    # ---------------------------------------------------------\n",
        "    prof = _get_profile()\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2) Title + short explanation\n",
        "    # ---------------------------------------------------------\n",
        "    gr.Markdown(\"## üéÆ Farm Rewards\")\n",
        "    gr.Markdown(\n",
        "        \"Complete daily missions to earn points and **1 spin per mission**. \"\n",
        "        \"Spin the wheel to win bonus points or vouchers (demo).\"\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3) KPI section (Points + Spins)\n",
        "    # ---------------------------------------------------------\n",
        "    with gr.Row():\n",
        "        points_box = gr.Textbox(\n",
        "            label=\"Points\",\n",
        "            value=str(prof.get(\"points\", 0)),\n",
        "            interactive=False\n",
        "        )\n",
        "        spins_box = gr.Textbox(\n",
        "            label=\"Spins available\",\n",
        "            value=str(prof.get(\"spins_available\", 0)),\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4) Missions + Vouchers display\n",
        "    # ---------------------------------------------------------\n",
        "    missions_md = gr.Markdown(_format_missions_md(prof))\n",
        "    coupons_txt = gr.Textbox(\n",
        "        label=\"Vouchers\",\n",
        "        value=_format_coupons_text(prof),\n",
        "        lines=7,\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5) Actions: Refresh + Spin\n",
        "    # ---------------------------------------------------------\n",
        "    with gr.Row():\n",
        "        refresh_btn = gr.Button(\"üîÑ Refresh\", variant=\"secondary\")\n",
        "        spin_btn = gr.Button(\"üé° Spin the Wheel\", variant=\"primary\")\n",
        "\n",
        "    spin_result = gr.Textbox(\n",
        "        label=\"Spin result\",\n",
        "        lines=2,\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 6) Redeem voucher by points (tier selection)\n",
        "    # ---------------------------------------------------------\n",
        "    gr.Markdown(\"### Redeem a Voucher\")\n",
        "\n",
        "    tier = gr.Dropdown(\n",
        "        choices=list(REDEEM_TIERS.keys()),\n",
        "        label=\"Select voucher tier\",\n",
        "        value=\"5‚Ç™ Voucher (100 pts)\"\n",
        "    )\n",
        "\n",
        "    redeem_btn = gr.Button(\"üéüÔ∏è Redeem\", variant=\"primary\")\n",
        "\n",
        "    redeem_result = gr.Textbox(\n",
        "        label=\"Redeem status\",\n",
        "        lines=2,\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 7) Wiring (connect buttons -> LOGIC handlers)\n",
        "    # ---------------------------------------------------------\n",
        "    refresh_btn.click(\n",
        "        fn=rewards_refresh,\n",
        "        outputs=[points_box, spins_box, missions_md, coupons_txt]\n",
        "    )\n",
        "\n",
        "    spin_btn.click(\n",
        "        fn=rewards_spin,\n",
        "        outputs=[spin_result, points_box, spins_box, missions_md, coupons_txt]\n",
        "    )\n",
        "\n",
        "    redeem_btn.click(\n",
        "        fn=rewards_redeem,\n",
        "        inputs=[tier],\n",
        "        outputs=[redeem_result, points_box, spins_box, missions_md, coupons_txt]\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 8) Return component refs (for build_app auto-refresh)\n",
        "    # ---------------------------------------------------------\n",
        "    return points_box, spins_box, missions_md, coupons_txt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFEXSm_SUr4-"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ ◊®◊©◊ô◊û◊™ ◊î◊ò◊ê◊ë◊ô◊ù ‚Äî ◊ñ◊î ◊î◊û◊ß◊ï◊ù ◊î◊ô◊ó◊ô◊ì ◊©◊û◊ï◊°◊ô◊§◊ô◊ù/◊û◊ï◊®◊ô◊ì◊ô◊ù ◊ò◊ê◊ë◊ô◊ù\n",
        "TABS = [\n",
        "    (\"üå± Realtime Dashboard\", build_realtime_dashboard_tab),\n",
        "    (\"üìä Analistic Dashboard\", build_iot_dashboard_tab),\n",
        "    (\"üìÑ Generate Report\", build_generate_report_tab),\n",
        "    (\"üñºÔ∏è Plant Disease Detection\", build_plant_disease_detection_tab),\n",
        "    (\"üí¨ RAG Chat\", build_rag_chat_tab),\n",
        "    (\"üí¨ Gemini Chat\", build_gemini_chat_tab),\n",
        "    (\"üîÑ Sync Data\", build_sync_data_tab),\n",
        "    (\"üéÆ Farm Rewards\", build_rewards_tab),\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33OVXQN6PXGG"
      },
      "source": [
        "## üìà Visualization Functions (Analistic Dashboard)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHqZANXcPWCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e1fb40-7a24-4a88-da1d-b3866443aa95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Sensor config loaded\n",
            "‚úì Component functions loaded\n",
            "‚úì Statistics functions loaded\n",
            "‚úì Chart styling loaded\n",
            "‚úÖ ALL visualization functions loaded!\n",
            "   ‚úì Time Series\n",
            "   ‚úì Correlations\n",
            "   ‚úì Hourly/Daily Patterns\n",
            "   ‚úì Histograms (Distributions)\n",
            "   ‚úì Moving Averages\n",
            "   ‚úì Statistics Cards\n",
            "   ‚úì Dashboard functions\n",
            "   ‚ùå Scatter Plots (REMOVED)\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Cell 7: Complete Visualization Functions (WITHOUT SCATTER ANALYSIS)\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "# Sensor configuration\n",
        "SENSORS = [\n",
        "    ('temperature', '¬∞C', COLORS['temperature']['color'], COLORS['temperature']['color'], 'TEMPERATURE'),\n",
        "    ('humidity', '%', COLORS['humidity']['color'], COLORS['humidity']['color'], 'HUMIDITY'),\n",
        "    ('soil', '%', COLORS['soil']['color'], COLORS['soil']['color'], 'SOIL MOISTURE')\n",
        "]\n",
        "print('‚úì Sensor config loaded')\n",
        "\n",
        "# ============================================================================\n",
        "# COMPONENT FUNCTIONS\n",
        "# ============================================================================\n",
        "def create_kpi_card(label, value, unit, change, change_label, trend=\"up\", border_color=None):\n",
        "    \"\"\"Create KPI card HTML.\"\"\"\n",
        "    bc = border_color or COLORS['temperature']['color']\n",
        "    icon = \"‚Üë\" if trend == \"up\" else (\"‚Üì\" if trend == \"down\" else \"‚Üí\")\n",
        "    return f'''<div class=\"kpi-card\" style=\"border-left-color: {bc};\">\n",
        "        <p class=\"kpi-label\">{label}</p>\n",
        "        <p class=\"kpi-value\">{value}<span style=\"font-size: 24px;\">{unit}</span></p>\n",
        "        <p class=\"kpi-change trend-{trend}\"><span>{icon}</span><span>{change} {change_label}</span></p>\n",
        "    </div>'''\n",
        "\n",
        "def create_status_badge(text=\"LIVE\", pulse=True):\n",
        "    \"\"\"Create status badge HTML.\"\"\"\n",
        "    pulse_dot = \"<span class='status-dot'></span>\" if pulse else \"\"\n",
        "    return f'<span class=\"status-badge\">{pulse_dot}{text}</span>'\n",
        "\n",
        "def create_explanation_card(title, description, interpretation, gradient=None):\n",
        "    \"\"\"Create explanation card HTML.\"\"\"\n",
        "    grad = gradient or COLORS['temperature']['color']\n",
        "    return f'''<div class=\"explanation-card\" style=\"background: {grad};\">\n",
        "        <h3>üìä {title}</h3><p><strong>What it shows:</strong> {description}</p>\n",
        "        <p><strong>How to interpret:</strong> {interpretation}</p></div>'''\n",
        "\n",
        "print('‚úì Component functions loaded')\n",
        "\n",
        "# ============================================================================\n",
        "# STATISTICS CARDS\n",
        "# ============================================================================\n",
        "def create_stat_cards_html(df):\n",
        "    \"\"\"Create comprehensive statistics cards for all sensors.\"\"\"\n",
        "    if len(df) == 0:\n",
        "        return \"<p>No data available</p>\"\n",
        "\n",
        "    explanations = {\n",
        "        'Mean': 'Average value. Sum √∑ count.',\n",
        "        'Median': 'Middle value. 50% above, 50% below.',\n",
        "        'Std Dev': 'Variability around mean. Low=consistent, High=variable.',\n",
        "        'Min': 'Lowest recorded value.',\n",
        "        'Max': 'Highest recorded value.',\n",
        "        'Q25': '25th percentile. 25% below this.',\n",
        "        'Q75': '75th percentile. 75% below this.',\n",
        "        'IQR': 'Interquartile range (Q75-Q25).'\n",
        "    }\n",
        "\n",
        "    sensor_colors = {\n",
        "        'TEMPERATURE': {'bg': '#fee2e2', 'text': '#991b1b', 'border': '#ef4444'},\n",
        "        'HUMIDITY': {'bg': '#dbeafe', 'text': '#1e40af', 'border': '#3b82f6'},\n",
        "        'SOIL MOISTURE': {'bg': '#e9d5ff', 'text': '#6b21a8', 'border': '#8b5cf6'}\n",
        "    }\n",
        "\n",
        "    html = '<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin: 20px 0;\">'\n",
        "\n",
        "    for var, unit, _, _, name in SENSORS:\n",
        "        stats = {k: round(v, 2) for k, v in {\n",
        "            'Mean': df[var].mean(),\n",
        "            'Median': df[var].median(),\n",
        "            'Std Dev': df[var].std(),\n",
        "            'Min': df[var].min(),\n",
        "            'Max': df[var].max(),\n",
        "            'Q25': df[var].quantile(0.25),\n",
        "            'Q75': df[var].quantile(0.75),\n",
        "            'IQR': df[var].quantile(0.75) - df[var].quantile(0.25)\n",
        "        }.items()}\n",
        "\n",
        "        colors = sensor_colors[name]\n",
        "\n",
        "        html += f'''\n",
        "        <div style=\"\n",
        "            background: {colors['bg']};\n",
        "            border: 3px solid {colors['border']};\n",
        "            border-radius: 12px;\n",
        "            padding: 20px;\n",
        "            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
        "        \">\n",
        "            <h2 style=\"\n",
        "                color: {colors['text']};\n",
        "                margin: 0 0 16px 0;\n",
        "                font-size: 20px;\n",
        "                font-weight: 700;\n",
        "                text-align: center;\n",
        "            \">{name}</h2>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 12px;\">\n",
        "        '''\n",
        "\n",
        "        for stat_name, stat_val in stats.items():\n",
        "            html += f'''\n",
        "            <div style=\"\n",
        "                background: white;\n",
        "                border-radius: 8px;\n",
        "                padding: 12px;\n",
        "                box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
        "            \">\n",
        "                <div style=\"\n",
        "                    display: flex;\n",
        "                    justify-content: space-between;\n",
        "                    align-items: center;\n",
        "                    margin-bottom: 4px;\n",
        "                \">\n",
        "                    <div style=\"\n",
        "                        font-size: 12px;\n",
        "                        font-weight: 600;\n",
        "                        color: {colors['text']};\n",
        "                    \">{stat_name}</div>\n",
        "                    <div class=\"info-icon\" style=\"\n",
        "                        width: 18px;\n",
        "                        height: 18px;\n",
        "                        border-radius: 50%;\n",
        "                        background: {colors['border']};\n",
        "                        color: white;\n",
        "                        display: flex;\n",
        "                        align-items: center;\n",
        "                        justify-content: center;\n",
        "                        font-size: 11px;\n",
        "                        font-weight: 700;\n",
        "                    \">i\n",
        "                        <span class=\"tooltip-text\">{explanations[stat_name]}</span>\n",
        "                    </div>\n",
        "                </div>\n",
        "                <div style=\"\n",
        "                    font-size: 24px;\n",
        "                    font-weight: 700;\n",
        "                    color: {colors['text']};\n",
        "                \">{stat_val}{unit}</div>\n",
        "            </div>\n",
        "            '''\n",
        "\n",
        "        html += '</div></div>'\n",
        "\n",
        "    html += '</div>'\n",
        "    return html\n",
        "\n",
        "print('‚úì Statistics functions loaded')\n",
        "\n",
        "# ============================================================================\n",
        "# CHART STYLING\n",
        "# ============================================================================\n",
        "def apply_chart_styling(fig, title=\"\", xaxis_title=\"\", yaxis_title=\"\", height=400):\n",
        "    \"\"\"Apply consistent styling to all charts.\"\"\"\n",
        "    fig.update_layout(\n",
        "        title=dict(text=title, font=dict(size=20)),\n",
        "        xaxis_title=xaxis_title,\n",
        "        yaxis_title=yaxis_title,\n",
        "        font=dict(family=\"Inter, sans-serif\", size=14),\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='white',\n",
        "        height=height,\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "    fig.update_xaxes(showgrid=False, title_font=dict(size=14, color='#6b7280'), tickfont=dict(size=12))\n",
        "    fig.update_yaxes(showgrid=True, gridcolor='#E5E7EB', title_font=dict(size=14, color='#6b7280'), tickfont=dict(size=12))\n",
        "    return fig\n",
        "\n",
        "print('‚úì Chart styling loaded')\n",
        "\n",
        "# ============================================================================\n",
        "# PLOT FUNCTIONS\n",
        "# ============================================================================\n",
        "def time_series_overview(df):\n",
        "    \"\"\"Time series for all sensors.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    for col, unit, color, _, _ in SENSORS:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df['timestamp'],\n",
        "            y=df[col],\n",
        "            name=col.capitalize(),\n",
        "            mode='lines',\n",
        "            line=dict(color=color, width=2),\n",
        "            hovertemplate=f'%{{y:.1f}}{unit}<extra></extra>'\n",
        "        ))\n",
        "    apply_chart_styling(fig, \"Sensor Data Time Series\", \"Time\", \"Measurement (¬∞C / %)\", 500)\n",
        "    return create_explanation_card(\n",
        "        \"Time Series Overview\",\n",
        "        \"All sensor measurements over time.\",\n",
        "        \"Look for trends, cycles, and sudden changes.\"\n",
        "    ), fig\n",
        "\n",
        "def calculate_correlations(df):\n",
        "    \"\"\"Correlation matrix between sensors.\"\"\"\n",
        "    corr = df[['temperature', 'humidity', 'soil']].corr()\n",
        "    fig = px.imshow(\n",
        "        corr,\n",
        "        labels=dict(color=\"Correlation\"),\n",
        "        x=['Temperature', 'Humidity', 'Soil'],\n",
        "        y=['Temperature', 'Humidity', 'Soil'],\n",
        "        color_continuous_scale='RdBu_r',\n",
        "        zmin=-1,\n",
        "        zmax=1,\n",
        "        aspect=\"auto\"\n",
        "    )\n",
        "    apply_chart_styling(fig, \"Correlation Matrix\", \"Variables\", \"Variables\", 500)\n",
        "\n",
        "    # Add correlation values\n",
        "    for i in range(len(corr)):\n",
        "        for j in range(len(corr)):\n",
        "            fig.add_annotation(\n",
        "                x=j, y=i,\n",
        "                text=str(round(corr.iloc[i, j], 3)),\n",
        "                showarrow=False,\n",
        "                font=dict(size=14, color='black' if abs(corr.iloc[i, j]) < 0.5 else 'white', weight=600)\n",
        "            )\n",
        "\n",
        "    return create_explanation_card(\n",
        "        \"Correlation Analysis\",\n",
        "        \"Linear relationships between sensors. +1=perfect positive, -1=perfect negative, 0=no relationship.\",\n",
        "        \"High correlations indicate sensors respond together.\",\n",
        "        COLORS['humidity']['color']\n",
        "    ), fig\n",
        "\n",
        "def hourly_patterns(df):\n",
        "    \"\"\"Average values by hour of day.\"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['hour'] = df_copy['timestamp'].dt.hour\n",
        "    hourly = df_copy.groupby('hour')[['temperature', 'humidity', 'soil']].mean()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    for col, _, color, _, _ in SENSORS:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=hourly.index,\n",
        "            y=hourly[col],\n",
        "            name=col.capitalize(),\n",
        "            mode='lines+markers',\n",
        "            line=dict(color=color, width=2.5),\n",
        "            marker=dict(size=8)\n",
        "        ))\n",
        "\n",
        "    apply_chart_styling(fig, \"Average Values by Hour\", \"Hour (0-23)\", \"Average Measurement (¬∞C / %)\", 450)\n",
        "    return create_explanation_card(\n",
        "        \"Hourly Patterns\",\n",
        "        \"Average values per hour showing daily cycles.\",\n",
        "        \"Look for peaks and valleys that repeat daily.\",\n",
        "        COLORS['soil']['color']\n",
        "    ), fig\n",
        "\n",
        "def daily_patterns(df):\n",
        "    \"\"\"Daily trends with min-max ranges.\"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['date'] = df_copy['timestamp'].dt.date\n",
        "    daily = df_copy.groupby('date')[['temperature', 'humidity', 'soil']].agg(['mean', 'min', 'max'])\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=1,\n",
        "        subplot_titles=('Temperature (¬∞C)', 'Humidity (%)', 'Soil (%)'),\n",
        "        vertical_spacing=0.08\n",
        "    )\n",
        "\n",
        "    for idx, (var, _, color, _, _) in enumerate(SENSORS, 1):\n",
        "        dates = [str(d) for d in daily.index]\n",
        "        r, g, b = int(color[1:3], 16), int(color[3:5], 16), int(color[5:7], 16)\n",
        "\n",
        "        # Min-max range\n",
        "        fig.add_trace(go.Scatter(x=dates, y=daily[var]['max'], mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip'), row=idx, col=1)\n",
        "        fig.add_trace(go.Scatter(x=dates, y=daily[var]['min'], mode='lines', line=dict(width=0),\n",
        "                                fill='tonexty', fillcolor=f\"rgba({r},{g},{b},0.2)\", showlegend=False, hoverinfo='skip'), row=idx, col=1)\n",
        "\n",
        "        # Mean line\n",
        "        fig.add_trace(go.Scatter(x=dates, y=daily[var]['mean'], mode='lines+markers',\n",
        "                                line=dict(color=color, width=2.5), marker=dict(size=6), name='Mean', showlegend=(idx==1)), row=idx, col=1)\n",
        "\n",
        "    fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
        "    fig.update_yaxes(title_text=\"Temperature (¬∞C)\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Humidity (%)\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Soil Moisture (%)\", row=3, col=1)\n",
        "    fig.update_layout(height=900)\n",
        "\n",
        "    return create_explanation_card(\n",
        "        \"Daily Trends\",\n",
        "        \"Daily means with min-max ranges (shaded).\",\n",
        "        \"Wider shading = more variability. Look for trends and unusual days.\"\n",
        "    ), fig\n",
        "\n",
        "def distribution_analysis(df):\n",
        "    \"\"\"Histograms showing distribution of sensor values.\"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=3,\n",
        "        subplot_titles=('Temperature (¬∞C)', 'Humidity (%)', 'Soil Moisture (%)')\n",
        "    )\n",
        "\n",
        "    # Temperature histogram\n",
        "    temp_data = df['temperature'].values\n",
        "    temp_min, temp_max = temp_data.min(), temp_data.max()\n",
        "    temp_padding = (temp_max - temp_min) * 0.1\n",
        "    temp_bins = np.linspace(temp_min - temp_padding, temp_max + temp_padding, 31)\n",
        "    temp_counts, temp_edges = np.histogram(temp_data, bins=temp_bins)\n",
        "    temp_centers = (temp_edges[:-1] + temp_edges[1:]) / 2\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=temp_centers,\n",
        "        y=temp_counts,\n",
        "        name='Temperature',\n",
        "        marker_color=COLORS['temperature']['color'],\n",
        "        width=(temp_max - temp_min) / 30 * 0.9,\n",
        "        hovertemplate='%{x:.1f}¬∞C: %{y} readings<extra></extra>'\n",
        "    ), row=1, col=1)\n",
        "\n",
        "    # Humidity histogram\n",
        "    humidity_data = df['humidity'].values\n",
        "    humidity_min, humidity_max = humidity_data.min(), humidity_data.max()\n",
        "    humidity_padding = (humidity_max - humidity_min) * 0.1\n",
        "    humidity_bins = np.linspace(humidity_min - humidity_padding, humidity_max + humidity_padding, 31)\n",
        "    humidity_counts, humidity_edges = np.histogram(humidity_data, bins=humidity_bins)\n",
        "    humidity_centers = (humidity_edges[:-1] + humidity_edges[1:]) / 2\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=humidity_centers,\n",
        "        y=humidity_counts,\n",
        "        name='Humidity',\n",
        "        marker_color=COLORS['humidity']['color'],\n",
        "        width=(humidity_max - humidity_min) / 30 * 0.9,\n",
        "        hovertemplate='%{x:.1f}%: %{y} readings<extra></extra>'\n",
        "    ), row=1, col=2)\n",
        "\n",
        "    # Soil histogram\n",
        "    soil_data = df['soil'].values\n",
        "    soil_min, soil_max = soil_data.min(), soil_data.max()\n",
        "    soil_padding = (soil_max - soil_min) * 0.1\n",
        "    soil_bins = np.linspace(soil_min - soil_padding, soil_max + soil_padding, 31)\n",
        "    soil_counts, soil_edges = np.histogram(soil_data, bins=soil_bins)\n",
        "    soil_centers = (soil_edges[:-1] + soil_edges[1:]) / 2\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=soil_centers,\n",
        "        y=soil_counts,\n",
        "        name='Soil Moisture',\n",
        "        marker_color=COLORS['soil']['color'],\n",
        "        width=(soil_max - soil_min) / 30 * 0.9,\n",
        "        hovertemplate='%{x:.1f}%: %{y} readings<extra></extra>'\n",
        "    ), row=1, col=3)\n",
        "\n",
        "    # Configure axes\n",
        "    fig.update_xaxes(title_text=\"Temperature (¬∞C)\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Humidity (%)\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Soil Moisture (%)\", row=1, col=3)\n",
        "    fig.update_yaxes(title_text=\"Number of Readings\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Number of Readings\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Number of Readings\", row=1, col=3)\n",
        "\n",
        "    fig.update_layout(height=400, showlegend=False, plot_bgcolor='white', paper_bgcolor='white')\n",
        "\n",
        "    return create_explanation_card(\n",
        "        \"Distribution Analysis\",\n",
        "        \"Frequency of sensor values. Tall bars = common values, short bars = rare values.\",\n",
        "        \"Look for the shape: bell curve = normal, multiple peaks = different patterns.\",\n",
        "        COLORS['humidity']['color']\n",
        "    ), fig\n",
        "\n",
        "def time_series_decomposition(df, variable='temperature'):\n",
        "    \"\"\"Moving averages showing smoothed trends.\"\"\"\n",
        "    df_s = df.sort_values('timestamp').copy()\n",
        "\n",
        "    # Calculate moving averages with different windows\n",
        "    for window in [3, 10, 30]:\n",
        "        df_s[f'MA_{window}'] = df_s[variable].rolling(window, center=True).mean()\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Raw data\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=df_s['timestamp'],\n",
        "        y=df_s[variable],\n",
        "        name='Raw',\n",
        "        mode='lines',\n",
        "        line=dict(width=1, color='#4B5563'),\n",
        "        opacity=0.6\n",
        "    ))\n",
        "\n",
        "    # Moving averages\n",
        "    for ma, color, width in [('MA_3', '#10b981', 1.5), ('MA_10', '#3b82f6', 2.5), ('MA_30', '#ef4444', 3.5)]:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df_s['timestamp'],\n",
        "            y=df_s[ma],\n",
        "            name=ma,\n",
        "            line=dict(width=width, color=color)\n",
        "        ))\n",
        "\n",
        "    unit = '¬∞C' if variable == 'temperature' else '%'\n",
        "    apply_chart_styling(fig, f'Moving Averages - {variable.capitalize()}', 'Time', f'{variable.capitalize()} ({unit})', 450)\n",
        "\n",
        "    return create_explanation_card(\n",
        "        \"Moving Averages\",\n",
        "        \"Smoothed trends at different scales (3, 10, 30 measurements).\",\n",
        "        \"Thicker lines=longer windows=smoother trends.\"\n",
        "    ), fig\n",
        "\n",
        "# ============================================================================\n",
        "# DASHBOARD FUNCTIONS FOR GRADIO\n",
        "# ============================================================================\n",
        "\n",
        "def create_kpi_cards(df):\n",
        "    \"\"\"Create simple KPI cards for dashboard.\"\"\"\n",
        "    if df.empty:\n",
        "        return \"<div style='padding: 20px; text-align: center;'>◊ê◊ô◊ü ◊†◊™◊ï◊†◊ô◊ù ◊ñ◊û◊ô◊†◊ô◊ù</div>\"\n",
        "\n",
        "    latest = df.iloc[-1]\n",
        "\n",
        "    # Calculate trends\n",
        "    if len(df) > 10:\n",
        "        prev = df.iloc[-10]\n",
        "        temp_trend = \"up\" if latest['temperature'] > prev['temperature'] else \"down\" if latest['temperature'] < prev['temperature'] else \"stable\"\n",
        "        hum_trend = \"up\" if latest['humidity'] > prev['humidity'] else \"down\" if latest['humidity'] < prev['humidity'] else \"stable\"\n",
        "        soil_trend = \"up\" if latest['soil'] > prev['soil'] else \"down\" if latest['soil'] < prev['soil'] else \"stable\"\n",
        "\n",
        "        temp_change = f\"{abs(latest['temperature'] - prev['temperature']):.1f}\"\n",
        "        hum_change = f\"{abs(latest['humidity'] - prev['humidity']):.1f}\"\n",
        "        soil_change = f\"{abs(latest['soil'] - prev['soil']):.1f}\"\n",
        "    else:\n",
        "        temp_trend = hum_trend = soil_trend = \"stable\"\n",
        "        temp_change = hum_change = soil_change = \"0.0\"\n",
        "\n",
        "    # Create HTML\n",
        "    html = f\"\"\"\n",
        "    <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 24px; margin: 20px 0;\">\n",
        "        {create_kpi_card('üå°Ô∏è Temperature', f'{latest[\"temperature\"]:.1f}', '¬∞C', temp_change, 'from last 10', temp_trend, COLORS['temperature']['color'])}\n",
        "        {create_kpi_card('üíß Humidity', f'{latest[\"humidity\"]:.1f}', '%', hum_change, 'from last 10', hum_trend, COLORS['humidity']['color'])}\n",
        "        {create_kpi_card('üå± Soil Moisture', f'{latest[\"soil\"]:.1f}', '%', soil_change, 'from last 10', soil_trend, COLORS['soil']['color'])}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return html\n",
        "\n",
        "def create_time_series_plot(df):\n",
        "    \"\"\"Create time series plot for dashboard.\"\"\"\n",
        "    if df.empty:\n",
        "        fig = go.Figure()\n",
        "        fig.add_annotation(\n",
        "            text=\"No data available\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5, showarrow=False,\n",
        "            font=dict(size=20)\n",
        "        )\n",
        "        fig.update_layout(height=500)\n",
        "        return fig\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=1,\n",
        "        subplot_titles=('üå°Ô∏è Temperature (¬∞C)', 'üíß Humidity (%)', 'üå± Soil Moisture (%)'),\n",
        "        vertical_spacing=0.08\n",
        "    )\n",
        "\n",
        "    # Temperature\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df['timestamp'],\n",
        "            y=df['temperature'],\n",
        "            name='Temperature',\n",
        "            line=dict(color=COLORS['temperature']['color'], width=2),\n",
        "            fill='tozeroy',\n",
        "            fillcolor=f\"rgba(239, 68, 68, 0.1)\"\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Humidity\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df['timestamp'],\n",
        "            y=df['humidity'],\n",
        "            name='Humidity',\n",
        "            line=dict(color=COLORS['humidity']['color'], width=2),\n",
        "            fill='tozeroy',\n",
        "            fillcolor=f\"rgba(59, 130, 246, 0.1)\"\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Soil\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df['timestamp'],\n",
        "            y=df['soil'],\n",
        "            name='Soil',\n",
        "            line=dict(color=COLORS['soil']['color'], width=2),\n",
        "            fill='tozeroy',\n",
        "            fillcolor=f\"rgba(139, 92, 246, 0.1)\"\n",
        "        ),\n",
        "        row=3, col=1\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_xaxes(showgrid=True, gridcolor='#E5E7EB')\n",
        "    fig.update_yaxes(showgrid=True, gridcolor='#E5E7EB')\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        showlegend=False,\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='white',\n",
        "        font=dict(family=\"Inter, sans-serif\")\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "print('‚úÖ ALL visualization functions loaded!')\n",
        "print('   ‚úì Time Series')\n",
        "print('   ‚úì Correlations')\n",
        "print('   ‚úì Hourly/Daily Patterns')\n",
        "print('   ‚úì Histograms (Distributions)')\n",
        "print('   ‚úì Moving Averages')\n",
        "print('   ‚úì Statistics Cards')\n",
        "print('   ‚úì Dashboard functions')\n",
        "print('   ‚ùå Scatter Plots (REMOVED)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M68MbKKPgOs"
      },
      "source": [
        "## üñ•Ô∏è Screen Functions (Analistic Dashboard & Sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VlalUqePjB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda49d50-0342-4a15-8949-5e629e409484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All screen functions loaded!\n",
            "   üìä Dashboard (without scatter)\n",
            "   üìâ Moving Averages\n",
            "   üîÑ Data Sync\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Cell 8: Screen Functions (WITHOUT SCATTER ANALYSIS)\n",
        "\n",
        "def sync_screen():\n",
        "    \"\"\"Sync data screen.\"\"\"\n",
        "    msg, count = sync_new_data_from_server()\n",
        "    return msg\n",
        "\n",
        "def dashboard_screen():\n",
        "    \"\"\"Load all data and return comprehensive dashboard (WITHOUT SCATTER).\"\"\"\n",
        "    df = load_data_from_firebase()\n",
        "\n",
        "    if df.empty:\n",
        "        empty_msg = \"<div style='padding: 20px; text-align: center;'>◊ê◊ô◊ü ◊†◊™◊ï◊†◊ô◊ù. ◊ú◊ó◊• ◊¢◊ú Sync Data!</div>\"\n",
        "        return empty_msg, None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Generate all visualizations (WITHOUT SCATTER)\n",
        "    kpi = create_kpi_cards(df)\n",
        "    stats = create_stat_cards_html(df)\n",
        "    ts = create_time_series_plot(df)\n",
        "    corr_card, corr_plot = calculate_correlations(df)\n",
        "    hourly_card, hourly_plot = hourly_patterns(df)\n",
        "    daily_card, daily_plot = daily_patterns(df)\n",
        "    dist_card, dist_plot = distribution_analysis(df)\n",
        "\n",
        "    return kpi, stats, ts, corr_card, corr_plot, hourly_card, hourly_plot, daily_card, daily_plot, dist_card, dist_plot\n",
        "\n",
        "def dashboard_moving_avg(variable):\n",
        "    \"\"\"Generate moving average plot for selected variable.\"\"\"\n",
        "    df = load_data_from_firebase()\n",
        "    if df.empty:\n",
        "        return None, \"◊ê◊ô◊ü ◊†◊™◊ï◊†◊ô◊ù\"\n",
        "    ma_card, ma_plot = time_series_decomposition(df, variable)\n",
        "    return ma_card, ma_plot\n",
        "\n",
        "print('‚úÖ All screen functions loaded!')\n",
        "print('   üìä Dashboard (without scatter)')\n",
        "print('   üìâ Moving Averages')\n",
        "print('   üîÑ Data Sync')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcqPbzgaOxyl"
      },
      "source": [
        "‚úÖ APP BUILDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXglJeFNO2rU"
      },
      "outputs": [],
      "source": [
        "def build_app():\n",
        "    with gr.Blocks(css=CUSTOM_CSS, title=\"Cloud Garden - IoT & AI\") as demo:\n",
        "        gr.Markdown(\"# üåø Cloud Garden - IoT & AI\")\n",
        "\n",
        "        rewards_tab_ref = None\n",
        "        rewards_outputs = None\n",
        "\n",
        "        with gr.Tabs():\n",
        "            for tab_name, tab_builder in TABS:\n",
        "                if tab_name.startswith(\"üéÆ\"):\n",
        "                    with gr.Tab(tab_name) as rewards_tab_ref:\n",
        "                        rewards_outputs = tab_builder()\n",
        "                else:\n",
        "                    with gr.Tab(tab_name):\n",
        "                        tab_builder()\n",
        "\n",
        "        # auto-refresh rewards on load + on tab select\n",
        "        if rewards_tab_ref and rewards_outputs:\n",
        "            points_box, spins_box, missions_md, coupons_txt = rewards_outputs\n",
        "\n",
        "            demo.load(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n",
        "            rewards_tab_ref.select(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n",
        "\n",
        "    return demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtxmPPB0O17q"
      },
      "source": [
        "‚úÖ LAUNCH (ONLY ONE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg5JT_i5O7mq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "1d6b1879-ea02-4938-f221-0823e6d7de84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a8ea4b200e26ac8ddf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a8ea4b200e26ac8ddf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    app = build_app()\n",
        "    app.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "726ab947ca294c58ab93eaee9826f017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84be023da3aa417ca98ef6f34f13adec",
              "IPY_MODEL_064c7e935ce944eebd45e60b15ab489d",
              "IPY_MODEL_758386d0c2e64d46bec8f132ddbcf2a6"
            ],
            "layout": "IPY_MODEL_274ff0c1d2aa464c8d82300fae161199"
          }
        },
        "84be023da3aa417ca98ef6f34f13adec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b9b0572b39647efbbd1df07695a48ba",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_68e6d5f37abe47648648788e069e0ae8",
            "value": "config.json:‚Äá"
          }
        },
        "064c7e935ce944eebd45e60b15ab489d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a7c978b23d04313a85a1d2e6c10b1eb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c209eb7772c449a9b19c4b92b09e9f54",
            "value": 1
          }
        },
        "758386d0c2e64d46bec8f132ddbcf2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_042dcef774e74347967b23293328a131",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e03c6f4feadb43388d529cbf3e6f34f2",
            "value": "‚Äá3.57k/?‚Äá[00:00&lt;00:00,‚Äá167kB/s]"
          }
        },
        "274ff0c1d2aa464c8d82300fae161199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b9b0572b39647efbbd1df07695a48ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68e6d5f37abe47648648788e069e0ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a7c978b23d04313a85a1d2e6c10b1eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c209eb7772c449a9b19c4b92b09e9f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "042dcef774e74347967b23293328a131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e03c6f4feadb43388d529cbf3e6f34f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fda48b9e2974569a0923c88e90b70b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_443965819c2d4c169e4f669ded6fa253",
              "IPY_MODEL_b1ce56b70ab5482f82c61c840c97a99a",
              "IPY_MODEL_31e1ea2f904142908e3a3f4baa64d442"
            ],
            "layout": "IPY_MODEL_749925a9650f443fa460ec5086d2d0c8"
          }
        },
        "443965819c2d4c169e4f669ded6fa253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4198ddd5cddb46cb86e2e66233f9314e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42193c69ee4146eca44720b135a888e1",
            "value": "pytorch_model.bin:‚Äá100%"
          }
        },
        "b1ce56b70ab5482f82c61c840c97a99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223d2ec616274bdd955d5ef4fcdedfd7",
            "max": 9335093,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90d8fbceefc34347b1275add4f79511d",
            "value": 9335093
          }
        },
        "31e1ea2f904142908e3a3f4baa64d442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eea0809ed6440098d172b15de410a3d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6d282e036f1041dd902818fc101e21d6",
            "value": "‚Äá9.34M/9.34M‚Äá[00:01&lt;00:00,‚Äá20.7kB/s]"
          }
        },
        "749925a9650f443fa460ec5086d2d0c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4198ddd5cddb46cb86e2e66233f9314e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42193c69ee4146eca44720b135a888e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "223d2ec616274bdd955d5ef4fcdedfd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d8fbceefc34347b1275add4f79511d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1eea0809ed6440098d172b15de410a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d282e036f1041dd902818fc101e21d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e7bd1e1b65440db94631964cc87dac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c594239619b410e8d29ecbb72e3690e",
              "IPY_MODEL_ad0d7ffae00c4e9dafb9dfd9ebb458f4",
              "IPY_MODEL_421024c3b59c4721b266d8951e2124d2"
            ],
            "layout": "IPY_MODEL_3ceca0ed139644f5931a71718ffe2c8a"
          }
        },
        "2c594239619b410e8d29ecbb72e3690e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf7202ec49b04ae79b23283298590b33",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5b46a4e5d81d4d3a9cf0f3be5afab6c0",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "ad0d7ffae00c4e9dafb9dfd9ebb458f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0a083473fc648a4a86b0e6e3466fb70",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f862738dbc04427abbbb222b8eb34b0",
            "value": 408
          }
        },
        "421024c3b59c4721b266d8951e2124d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c58ebb4a1474e9b85ec8a0e35cf31f9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f1cd76e2ea85440fa806169eb9371b2b",
            "value": "‚Äá408/408‚Äá[00:00&lt;00:00,‚Äá9.01kB/s]"
          }
        },
        "3ceca0ed139644f5931a71718ffe2c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7202ec49b04ae79b23283298590b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b46a4e5d81d4d3a9cf0f3be5afab6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0a083473fc648a4a86b0e6e3466fb70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f862738dbc04427abbbb222b8eb34b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c58ebb4a1474e9b85ec8a0e35cf31f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1cd76e2ea85440fa806169eb9371b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c0f95ebb72b4e6f93c56e9e9a4a2ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f949420883b49cd9803752cc9d71a95",
              "IPY_MODEL_a93a6a45526748ea9b64301646d06010",
              "IPY_MODEL_0d330e0c46e545fa84c689ccd19e3632"
            ],
            "layout": "IPY_MODEL_ae8caf58b86a41348baed61f32fdfb90"
          }
        },
        "9f949420883b49cd9803752cc9d71a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5818ef8eef3342b18a312d1c6e30b683",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1f47d9634f584206a41a064251147fb0",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "a93a6a45526748ea9b64301646d06010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a598ae3615594b9ba8f9c4d67220bb6f",
            "max": 9264680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3085076ff89d4f32b2393875d89ce1fd",
            "value": 9264680
          }
        },
        "0d330e0c46e545fa84c689ccd19e3632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec8d79926064dc0a02328177404721f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_93c47a84e19d42eb9daf81775c1dba66",
            "value": "‚Äá9.26M/9.26M‚Äá[00:02&lt;00:00,‚Äá4.31MB/s]"
          }
        },
        "ae8caf58b86a41348baed61f32fdfb90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5818ef8eef3342b18a312d1c6e30b683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f47d9634f584206a41a064251147fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a598ae3615594b9ba8f9c4d67220bb6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3085076ff89d4f32b2393875d89ce1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fec8d79926064dc0a02328177404721f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c47a84e19d42eb9daf81775c1dba66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}